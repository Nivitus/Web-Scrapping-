{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\" lang=\"en-GB\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width\" name=\"viewport\"/>\n",
      "  <meta content=\"f1rF5Q46AlxKE9mgvLWSgJVXTDU\" name=\"alexaVerifyID\"/>\n",
      "  <meta content=\"EIpXG2ziBs1VxOnhQfd8Lc2coBZPzEYLFyb8Fv2j-d0\" name=\"google-site-verification\"/>\n",
      "  <link href=\"http://gmpg.org/xfn/11\" rel=\"profile\"/>\n",
      "  <link href=\"http://airesearch.com/xmlrpc.php\" rel=\"pingback\"/>\n",
      "  <!--[if lt IE 9]>\n",
      "\t<script src=\"http://airesearch.com/wp-content/themes/twentyfifteen/js/html5.js\"></script>\n",
      "\t<![endif]-->\n",
      "  <script>\n",
      "   (function(html){html.className = html.className.replace(/\\bno-js\\b/,'js')})(document.documentElement);\n",
      "  </script>\n",
      "  <title>\n",
      "   Research Papers – AI Research\n",
      "  </title>\n",
      "  <link href=\"http://airesearch.com/feed/\" rel=\"alternate\" title=\"AI Research » Feed\" type=\"application/rss+xml\"/>\n",
      "  <link href=\"http://airesearch.com/comments/feed/\" rel=\"alternate\" title=\"AI Research » Comments Feed\" type=\"application/rss+xml\"/>\n",
      "  <link href=\"http://airesearch.com/category/ai-research-papers/feed/\" rel=\"alternate\" title=\"AI Research » Research Papers Category Feed\" type=\"application/rss+xml\"/>\n",
      "  <script type=\"text/javascript\">\n",
      "   window._wpemojiSettings = {\"baseUrl\":\"http:\\/\\/s.w.org\\/images\\/core\\/emoji\\/72x72\\/\",\"ext\":\".png\",\"source\":{\"concatemoji\":\"http:\\/\\/airesearch.com\\/wp-includes\\/js\\/wp-emoji-release.min.js?ver=4.4.1\"}};\n",
      "\t\t\t!function(a,b,c){function d(a){var c,d=b.createElement(\"canvas\"),e=d.getContext&&d.getContext(\"2d\");return e&&e.fillText?(e.textBaseline=\"top\",e.font=\"600 32px Arial\",\"flag\"===a?(e.fillText(String.fromCharCode(55356,56806,55356,56826),0,0),d.toDataURL().length>3e3):\"diversity\"===a?(e.fillText(String.fromCharCode(55356,57221),0,0),c=e.getImageData(16,16,1,1).data.toString(),e.fillText(String.fromCharCode(55356,57221,55356,57343),0,0),c!==e.getImageData(16,16,1,1).data.toString()):(\"simple\"===a?e.fillText(String.fromCharCode(55357,56835),0,0):e.fillText(String.fromCharCode(55356,57135),0,0),0!==e.getImageData(16,16,1,1).data[0])):!1}function e(a){var c=b.createElement(\"script\");c.src=a,c.type=\"text/javascript\",b.getElementsByTagName(\"head\")[0].appendChild(c)}var f,g;c.supports={simple:d(\"simple\"),flag:d(\"flag\"),unicode8:d(\"unicode8\"),diversity:d(\"diversity\")},c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.simple&&c.supports.flag&&c.supports.unicode8&&c.supports.diversity||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener(\"DOMContentLoaded\",g,!1),a.addEventListener(\"load\",g,!1)):(a.attachEvent(\"onload\",g),b.attachEvent(\"onreadystatechange\",function(){\"complete\"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);\n",
      "  </script>\n",
      "  <style type=\"text/css\">\n",
      "   img.wp-smiley,\n",
      "img.emoji {\n",
      "\tdisplay: inline !important;\n",
      "\tborder: none !important;\n",
      "\tbox-shadow: none !important;\n",
      "\theight: 1em !important;\n",
      "\twidth: 1em !important;\n",
      "\tmargin: 0 .07em !important;\n",
      "\tvertical-align: -0.1em !important;\n",
      "\tbackground: none !important;\n",
      "\tpadding: 0 !important;\n",
      "}\n",
      "  </style>\n",
      "  <link href=\"https://fonts.googleapis.com/css?family=Noto+Sans%3A400italic%2C700italic%2C400%2C700%7CNoto+Serif%3A400italic%2C700italic%2C400%2C700%7CInconsolata%3A400%2C700&amp;subset=latin%2Clatin-ext\" id=\"twentyfifteen-fonts-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <link href=\"http://airesearch.com/wp-content/themes/twentyfifteen/genericons/genericons.css?ver=3.2\" id=\"genericons-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <link href=\"http://airesearch.com/wp-content/themes/twentyfifteen/style.css?ver=4.4.1\" id=\"twentyfifteen-style-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <!--[if lt IE 9]>\n",
      "<link rel='stylesheet' id='twentyfifteen-ie-css'  href='http://airesearch.com/wp-content/themes/twentyfifteen/css/ie.css?ver=20141010' type='text/css' media='all' />\n",
      "<![endif]-->\n",
      "  <!--[if lt IE 8]>\n",
      "<link rel='stylesheet' id='twentyfifteen-ie7-css'  href='http://airesearch.com/wp-content/themes/twentyfifteen/css/ie7.css?ver=20141010' type='text/css' media='all' />\n",
      "<![endif]-->\n",
      "  <script src=\"http://airesearch.com/wp-includes/js/jquery/jquery.js?ver=1.11.3\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script src=\"http://airesearch.com/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.2.1\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <link href=\"http://airesearch.com/wp-json/\" rel=\"https://api.w.org/\"/>\n",
      "  <link href=\"http://airesearch.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n",
      "  <link href=\"http://airesearch.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n",
      "  <meta content=\"WordPress 4.4.1\" name=\"generator\"/>\n",
      " </head>\n",
      " <body class=\"archive category category-ai-research-papers category-98\">\n",
      "  <div class=\"hfeed site\" id=\"page\">\n",
      "   <a class=\"skip-link screen-reader-text\" href=\"#content\">\n",
      "    Skip to content\n",
      "   </a>\n",
      "   <div class=\"sidebar\" id=\"sidebar\">\n",
      "    <header class=\"site-header\" id=\"masthead\" role=\"banner\">\n",
      "     <div class=\"site-branding\">\n",
      "      <p class=\"site-title\">\n",
      "       <a href=\"http://airesearch.com/\" rel=\"home\">\n",
      "        <img alt=\"AI Research\" src=\"http://airesearch.com/wp-content/uploads/2016/01/ai_research_site_logo.png\" title=\"AI Research\"/>\n",
      "       </a>\n",
      "      </p>\n",
      "      <p class=\"site-description\">\n",
      "       Thoughts, Theories &amp; Studies on Artificial Intelligence (AI) Research\n",
      "      </p>\n",
      "      <button class=\"secondary-toggle\">\n",
      "       Menu and widgets\n",
      "      </button>\n",
      "     </div>\n",
      "     <!-- .site-branding -->\n",
      "    </header>\n",
      "    <!-- .site-header -->\n",
      "    <div class=\"secondary\" id=\"secondary\">\n",
      "     <nav class=\"main-navigation\" id=\"site-navigation\" role=\"navigation\">\n",
      "      <div class=\"menu-main-container\">\n",
      "       <ul class=\"nav-menu\" id=\"menu-main\">\n",
      "        <li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-17\" id=\"menu-item-17\">\n",
      "         <a href=\"http://airesearch.com/category/ai-blog/\">\n",
      "          Blog Articles\n",
      "         </a>\n",
      "        </li>\n",
      "        <li class=\"menu-item menu-item-type-taxonomy menu-item-object-category current-menu-item menu-item-227\" id=\"menu-item-227\">\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </li>\n",
      "        <li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-19\" id=\"menu-item-19\">\n",
      "         <a href=\"http://airesearch.com/category/ai-talks/\">\n",
      "          Talks\n",
      "         </a>\n",
      "        </li>\n",
      "        <li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-18\" id=\"menu-item-18\">\n",
      "         <a href=\"http://airesearch.com/category/ai-books/\">\n",
      "          Books\n",
      "         </a>\n",
      "        </li>\n",
      "        <li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-139\" id=\"menu-item-139\">\n",
      "         <a href=\"http://airesearch.com/category/ai-groups/\">\n",
      "          People &amp; Groups\n",
      "         </a>\n",
      "        </li>\n",
      "        <li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-224\" id=\"menu-item-224\">\n",
      "         <a href=\"http://airesearch.com/category/ai-resources/\">\n",
      "          Dev Resources\n",
      "         </a>\n",
      "        </li>\n",
      "        <li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-226\" id=\"menu-item-226\">\n",
      "         <a href=\"http://airesearch.com/category/ai-programs/\">\n",
      "          Academic Programs\n",
      "         </a>\n",
      "        </li>\n",
      "       </ul>\n",
      "      </div>\n",
      "     </nav>\n",
      "     <!-- .main-navigation -->\n",
      "     <div class=\"widget-area\" id=\"widget-area\" role=\"complementary\">\n",
      "      <aside class=\"widget widget_search\" id=\"search-2\">\n",
      "       <h2 class=\"widget-title\">\n",
      "        Search AI Research\n",
      "       </h2>\n",
      "       <form action=\"http://airesearch.com/\" class=\"search-form\" method=\"get\" role=\"search\">\n",
      "        <label>\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Search for:\n",
      "         </span>\n",
      "         <input class=\"search-field\" name=\"s\" placeholder=\"Search …\" title=\"Search for:\" type=\"search\" value=\"\"/>\n",
      "        </label>\n",
      "        <input class=\"search-submit screen-reader-text\" type=\"submit\" value=\"Search\"/>\n",
      "       </form>\n",
      "      </aside>\n",
      "      <aside class=\"widget widget_text\" id=\"text-2\">\n",
      "       <h2 class=\"widget-title\">\n",
      "        Welcome!\n",
      "       </h2>\n",
      "       <div class=\"textwidget\">\n",
      "        <p>\n",
      "         AI Research [.com] is a collection of global thoughts, theories and studies on the subject of Artificial Intelligence.\n",
      "        </p>\n",
      "        <p>\n",
      "         Got something to add? Please submit it\n",
      "         <a href=\"http://airesearch.com/submit-something-about-ai/\">\n",
      "          here\n",
      "         </a>\n",
      "         .\n",
      "        </p>\n",
      "       </div>\n",
      "      </aside>\n",
      "     </div>\n",
      "     <!-- .widget-area -->\n",
      "    </div>\n",
      "    <!-- .secondary -->\n",
      "   </div>\n",
      "   <!-- .sidebar -->\n",
      "   <div class=\"site-content\" id=\"content\">\n",
      "    <section class=\"content-area\" id=\"primary\">\n",
      "     <main class=\"site-main\" id=\"main\" role=\"main\">\n",
      "      <header class=\"page-header\">\n",
      "       <h1 class=\"page-title\">\n",
      "        Category: Research Papers\n",
      "       </h1>\n",
      "      </header>\n",
      "      <!-- .page-header -->\n",
      "      <article class=\"post-357 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers\" id=\"post-357\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/explained-xlnet-generalized-autoregressive-pretraining-for-language-understanding/\">\n",
      "        <img alt=\"Explained: XLNet – Generalized Autoregressive Pretraining for Language Understanding\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2019/07/xlnet-825x510.png\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/explained-xlnet-generalized-autoregressive-pretraining-for-language-understanding/\" rel=\"bookmark\">\n",
      "          Explained: XLNet – Generalized Autoregressive Pretraining for Language Understanding\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         Carnegie Mellon and Google’s Brain outfit have tried to undo some of the techniques of Google’s BERT machine learning model for natural language processing.\n",
      "        </p>\n",
      "        <p>\n",
      "         They propose a new approach called “XLNet.” Built on top of the popular “Transformer” A.I. for language, it may be a more straightforward way to examine how language works.\n",
      "        </p>\n",
      "        <p>\n",
      "         XLNet is an exciting development in NLP, not only because of its results but because it shows us that there is still room to improve upon for transfer learning in NLP.\n",
      "        </p>\n",
      "        <p>\n",
      "         Machine Learning Explained’s article,\n",
      "         <a href=\"https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\" target=\"_blank\">\n",
      "          Paper Dissected: “XLNet: Generalized Autoregressive Pretraining for Language Understanding” Explained\n",
      "         </a>\n",
      "         , offers a clear summary of arguably one of 2019’s most important developments in Natural Language Processing.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Further Readings\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"https://arxiv.org/pdf/1906.08237.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">\n",
      "          The original paper\n",
      "         </a>\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/\" rel=\"noopener noreferrer\" target=\"_blank\">\n",
      "          Blog post on the Transformer\n",
      "         </a>\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"http://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/\" rel=\"noopener noreferrer\" target=\"_blank\">\n",
      "          Blog post on ELMo\n",
      "         </a>\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/\" rel=\"noopener noreferrer\" target=\"_blank\">\n",
      "          Blog post on BERT\n",
      "         </a>\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/explained-xlnet-generalized-autoregressive-pretraining-for-language-understanding/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published updated\" datetime=\"2019-07-21T07:07:11+00:00\">\n",
      "           July 21, 2019\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/explained-xlnet-generalized-autoregressive-pretraining-for-language-understanding/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on Explained: XLNet – Generalized Autoregressive Pretraining for Language Understanding\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-338 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers tag-aishwarya-agrawal tag-aniruddha-kembhavi tag-devi-parikh tag-dhruv-batra tag-facebook-ai-research\" id=\"post-338\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/overcoming-priors-for-visual-question-answering/\">\n",
      "        <img alt=\"Overcoming Priors for Visual Question Answering\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/12/facebook_ai_research-825x510.png\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/overcoming-priors-for-visual-question-answering/\" rel=\"bookmark\">\n",
      "          Overcoming Priors for Visual Question Answering\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <h3>\n",
      "         Abstract\n",
      "        </h3>\n",
      "        <p>\n",
      "         A number of studies have found that today’s Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from ‘cheating’ by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model – Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Aishwarya Agrawal\n",
      "         </strong>\n",
      "         ,\n",
      "         <strong>\n",
      "          <a class=\"author-link\" href=\"https://research.fb.com/people/batra-dhruv/\">\n",
      "           Dhruv Batra\n",
      "          </a>\n",
      "         </strong>\n",
      "         ,\n",
      "         <strong>\n",
      "          <a class=\"author-link\" href=\"https://research.fb.com/people/parikh-devi/\">\n",
      "           Devi Parikh\n",
      "          </a>\n",
      "         </strong>\n",
      "         ,\n",
      "         <strong>\n",
      "          Aniruddha Kembhavi\n",
      "          <br/>\n",
      "         </strong>\n",
      "         [Facebook AI Research]\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"https://research.fb.com/wp-content/uploads/2018/05/done28099t-just-assume-look-and-answer-overcoming-priors-for-visual-question-answering.pdf?\" target=\"_blank\">\n",
      "          Download the full paper here\n",
      "         </a>\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/overcoming-priors-for-visual-question-answering/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published updated\" datetime=\"2018-06-18T13:34:12+00:00\">\n",
      "           June 18, 2018\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"tags-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Tags\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/tag/aishwarya-agrawal/\" rel=\"tag\">\n",
      "          Aishwarya Agrawal\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/aniruddha-kembhavi/\" rel=\"tag\">\n",
      "          Aniruddha Kembhavi\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/devi-parikh/\" rel=\"tag\">\n",
      "          Devi Parikh\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/dhruv-batra/\" rel=\"tag\">\n",
      "          Dhruv Batra\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/facebook-ai-research/\" rel=\"tag\">\n",
      "          Facebook AI Research\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/overcoming-priors-for-visual-question-answering/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on Overcoming Priors for Visual Question Answering\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-334 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers tag-ari-s-morcos tag-david-g-t-barrett tag-deepmind tag-google-deepmind tag-matthew-botvinick tag-neil-c-rabinowitz\" id=\"post-334\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/the-importance-of-single-directions-for-generalization/\">\n",
      "        <img alt=\"The importance of single directions for generalization\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2016/10/deepmind-825x510.png\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/the-importance-of-single-directions-for-generalization/\" rel=\"bookmark\">\n",
      "          The importance of single directions for generalization\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         <strong>\n",
      "          ABSTRACT\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network’s reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Ari S. Morcos\n",
      "         </strong>\n",
      "         ,\n",
      "         <strong>\n",
      "          David G.T. Barrett\n",
      "         </strong>\n",
      "         ,\n",
      "         <strong>\n",
      "          Neil C. Rabinowitz\n",
      "         </strong>\n",
      "         , &amp;\n",
      "         <strong>\n",
      "          Matthew Botvinick\n",
      "          <br/>\n",
      "          @DeepMind\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"ttps://arxiv.org/pdf/1803.06959.pdf\" target=\"_blank\">\n",
      "          Download the full paper here\n",
      "         </a>\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/the-importance-of-single-directions-for-generalization/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published\" datetime=\"2018-03-26T14:01:00+00:00\">\n",
      "           March 26, 2018\n",
      "          </time>\n",
      "          <time class=\"updated\" datetime=\"2018-04-03T14:02:30+00:00\">\n",
      "           April 3, 2018\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"tags-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Tags\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/tag/ari-s-morcos/\" rel=\"tag\">\n",
      "          Ari S. Morcos\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/david-g-t-barrett/\" rel=\"tag\">\n",
      "          David G.T. Barrett\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/deepmind/\" rel=\"tag\">\n",
      "          DeepMind\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/google-deepmind/\" rel=\"tag\">\n",
      "          Google DeepMind\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/matthew-botvinick/\" rel=\"tag\">\n",
      "          Matthew Botvinick\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/neil-c-rabinowitz/\" rel=\"tag\">\n",
      "          Neil C. Rabinowitz\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/the-importance-of-single-directions-for-generalization/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on The importance of single directions for generalization\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-326 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers tag-ai tag-ben-goertzel tag-blockchain tag-singularity tag-singularitynet\" id=\"post-326\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/singularitynet-a-decentralized-open-market-and-inter-network-for-ais/\">\n",
      "        <img alt=\"SingularityNET: A decentralized, open market and inter-network for AIs\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2017/12/singularity-1-825x510.jpg\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/singularitynet-a-decentralized-open-market-and-inter-network-for-ais/\" rel=\"bookmark\">\n",
      "          SingularityNET: A decentralized, open market and inter-network for AIs\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         <strong>\n",
      "          ABSTRACT\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         The value and power of Artificial Intelligence is growing dramatically every year, and will soon dominate the internet – and the economy as a whole. However, AI tools today are fragmented by a closed development environment; most are developed by one company to perform one task, and there is no way to plug two tools together. SingularityNET aims to become the key protocol for networking AI and machine learning tools to form a coordinated Artificial General Intelligence. SingularityNET is an open-source protocol and collection of smart contracts for a decentralized market of coordinated AI services. Within this framework, the benefits of AI become a global commons infrastructure for the benefit of all; anyone can access AI tech or become a stakeholder in its development. Anyone can add an AI/machine learning service to SingularityNET for use by the network, and receive network payment tokens in exchange. SingularityNET is backed by the SingularityNET Foundation, which operates on a belief that the benefits of AI should not be dominated by any small set of powerful institutions, but shared by all. A key goal of SingularityNET is to ensure the technology is benevolent according to human standards, and the network is designed to incentivize and reward beneficial players.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Ben Goertzel\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"http://airesearch.com/wp-content/uploads/2017/12/SingularityNET.pdf\" target=\"_blank\">\n",
      "          Download the full paper here\n",
      "         </a>\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/singularitynet-a-decentralized-open-market-and-inter-network-for-ais/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published\" datetime=\"2017-12-25T13:44:48+00:00\">\n",
      "           December 25, 2017\n",
      "          </time>\n",
      "          <time class=\"updated\" datetime=\"2017-12-25T13:52:19+00:00\">\n",
      "           December 25, 2017\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"tags-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Tags\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/tag/ai/\" rel=\"tag\">\n",
      "          AI\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/ben-goertzel/\" rel=\"tag\">\n",
      "          Ben Goertzel\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/blockchain/\" rel=\"tag\">\n",
      "          Blockchain\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/singularity/\" rel=\"tag\">\n",
      "          Singularity\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/singularitynet/\" rel=\"tag\">\n",
      "          SingularityNET\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/singularitynet-a-decentralized-open-market-and-inter-network-for-ais/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on SingularityNET: A decentralized, open market and inter-network for AIs\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-308 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers\" id=\"post-308\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/wavenet-a-generative-model-for-raw-audio/\">\n",
      "        <img alt=\"WaveNet: A Generative Model for Raw Audio\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2016/10/deepmind-825x510.png\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/wavenet-a-generative-model-for-raw-audio/\" rel=\"bookmark\">\n",
      "          WaveNet: A Generative Model for Raw Audio\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         <strong>\n",
      "          ABSTRACT\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu\n",
      "          <br/>\n",
      "         </strong>\n",
      "         <em>\n",
      "          {avdnoord, sedielem, heigazen, simonyan, vinyals, gravesa, nalk, andrewsenior, korayk}@google.com\n",
      "         </em>\n",
      "         <br/>\n",
      "         <em>\n",
      "          Google DeepMind\n",
      "         </em>\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"https://arxiv.org/pdf/1609.03499.pdf\" target=\"_blank\">\n",
      "          Download the full paper here\n",
      "         </a>\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/wavenet-a-generative-model-for-raw-audio/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published\" datetime=\"2016-10-26T06:07:41+00:00\">\n",
      "           October 26, 2016\n",
      "          </time>\n",
      "          <time class=\"updated\" datetime=\"2016-10-26T06:18:00+00:00\">\n",
      "           October 26, 2016\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/wavenet-a-generative-model-for-raw-audio/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on WaveNet: A Generative Model for Raw Audio\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-186 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers tag-ai tag-ai-research tag-armand-joulin tag-facebook-ai-research tag-machine-intelligence tag-marco-baroni tag-tomas-mikolov\" id=\"post-186\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/facebook-ai-research-machine-intelligence-roadmap/\">\n",
      "        <img alt=\"Facebook AI Research – Machine Intelligence Roadmap\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/12/facebook_ai_research-825x510.png\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/facebook-ai-research-machine-intelligence-roadmap/\" rel=\"bookmark\">\n",
      "          Facebook AI Research – Machine Intelligence Roadmap\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         <strong>\n",
      "          Abstract\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Tomas Mikolov, Armand Joulin, Marco Baroni\n",
      "         </strong>\n",
      "         <br/>\n",
      "         <em>\n",
      "          Facebook AI Research\n",
      "         </em>\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          1 Introduction\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         A machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavour, from performing menial jobs for us to helping the advancement of basic and applied research. Given the current availability of powerful hardware and large amounts of machine-readable data, as well as the widespread interest in sophisticated machine learning methods, the times should be ripe for the development of intelligent machines.\n",
      "        </p>\n",
      "        <p>\n",
      "         We think that one fundamental reasons for this is that, since “solving AI” at once seems too complex a task to be pursued all at once, the computational community has preferred to focus, in the last decades, on solving relatively narrow empirical problems that are important for specific applications, but do not address the overarching goal of developing general-purpose intelligent machines.\n",
      "        </p>\n",
      "        <p>\n",
      "         In this article, we propose an alternative approach: we first define the general characteristics we think intelligent machines should possess, and then we present a concrete roadmap to develop them in realistic, small steps, that are however incrementally structured in such a way that, jointly, they should lead us close to the ultimate goal of implementing a powerful AI. We realise that our vision of artificial intelligence and how to create it is just one among many. We focus here on a plan that, we hope, will lead to genuine progress, without by this implying that there are not other valid approaches to the task.\n",
      "        </p>\n",
      "        <p>\n",
      "         The article is structured as follows. In Section 2 we indicate the two fundamental characteristics that we consider crucial for developing intelligence– at least the sort of intelligence we are interested in–namely communication and learning. Our goal is to build a machine that can learn new concepts through communication at a similar rate as a human with similar prior knowledge. That is, if one can easily learn how subtraction works after mastering addition, the intelligent machine, after grasping the concept of addition, should not find it difficult to learn subtraction as well.\n",
      "        </p>\n",
      "        <p>\n",
      "         Since, as we said, achieving the long-term goal of building an intelligent machine equipped with the desired features at once seems too difficult, we need to define intermediate targets that can lead us in the right direction. We specify such targets in terms of simplified but self-contained versions of the final machine we want to develop. Our plan is to “educate” the target machine like a child: At any time in its development, the target machine should act like a stand-alone intelligent system, albeit one that will be initially very limited in what it can do. The bulk of our proposal (Section 3) thus consists in the plan for an interactive learning environment fostering the incremental development of progressively more intelligent behaviour.\n",
      "        </p>\n",
      "        <p>\n",
      "         Section 4 briefly discusses some of the algorithmic capabilities we think a machine should possess in order to profitably exploit the learning environment. Finally, Section 5 situates our proposal in the broader context of past and current attempts to develop intelligent machines.\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"http://airesearch.com/wp-content/uploads/2016/01/A_Roadmap_towards_Machine_Intelligence.pdf\" target=\"_blank\">\n",
      "          Download the full paper here\n",
      "         </a>\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/facebook-ai-research-machine-intelligence-roadmap/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published\" datetime=\"2016-01-11T14:53:04+00:00\">\n",
      "           January 11, 2016\n",
      "          </time>\n",
      "          <time class=\"updated\" datetime=\"2016-01-26T13:27:49+00:00\">\n",
      "           January 26, 2016\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"tags-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Tags\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/tag/ai/\" rel=\"tag\">\n",
      "          AI\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/ai-research/\" rel=\"tag\">\n",
      "          AI Research\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/armand-joulin/\" rel=\"tag\">\n",
      "          Armand Joulin\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/facebook-ai-research/\" rel=\"tag\">\n",
      "          Facebook AI Research\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/machine-intelligence/\" rel=\"tag\">\n",
      "          Machine Intelligence\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/marco-baroni/\" rel=\"tag\">\n",
      "          Marco Baroni\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/tomas-mikolov/\" rel=\"tag\">\n",
      "          Tomas Mikolov\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/facebook-ai-research-machine-intelligence-roadmap/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on Facebook AI Research – Machine Intelligence Roadmap\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-172 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers tag-artificial-creativity tag-lovelace tag-mark-o-riedl\" id=\"post-172\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/the-lovelace-2-0-test-of-artificial-creativity-and-intelligence/\">\n",
      "        <img alt=\"The Lovelace 2.0 Test of Artificial Creativity and Intelligence\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2016/01/ai_art-825x510.jpg\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/the-lovelace-2-0-test-of-artificial-creativity-and-intelligence/\" rel=\"bookmark\">\n",
      "          The Lovelace 2.0 Test of Artificial Creativity and Intelligence\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         The Lovelace 2.0 Test asks whether a computer can create an artefact – a poem, story, painting or architectural design – that expert and unbiased observers would conclude was designed by a human.\n",
      "        </p>\n",
      "        <p>\n",
      "         Prof. Mark Riedl proposes the concept of artificial creativity, akin to artificial intelligence. This could be tested he says, using an alternative to the Turing Test, the AI benchmark that asserts that if a computer system can fool a human being into thinking it is human itself, then it can be said to be truly intelligent.\n",
      "        </p>\n",
      "        <p>\n",
      "         You can read Prof. Riedl’s paper below:\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Abstract\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         Observing that the creation of certain types of artistic artefacts necessitate intelligence, we present the Lovelace 2.0 Test of creativity as an alternative to the Turing Test as a means of determining whether an agent is intelligent.\n",
      "        </p>\n",
      "        <p>\n",
      "         The Lovelace 2.0 Test builds off prior tests of creativity and additionally provides a means of directly comparing the relative intelligence of different agents.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Mark O. Riedl\n",
      "         </strong>\n",
      "         <br/>\n",
      "         School of Interactive Computing; Georgia Institute of Technology\n",
      "         <br/>\n",
      "         riedl@cc.gatech.edu\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"http://airesearch.com/wp-content/uploads/2016/01/1410.6142v1.pdf\" target=\"_blank\">\n",
      "          Download the full paper here\n",
      "         </a>\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/the-lovelace-2-0-test-of-artificial-creativity-and-intelligence/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published\" datetime=\"2016-01-05T17:47:39+00:00\">\n",
      "           January 5, 2016\n",
      "          </time>\n",
      "          <time class=\"updated\" datetime=\"2016-01-26T13:27:20+00:00\">\n",
      "           January 26, 2016\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"tags-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Tags\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/tag/artificial-creativity/\" rel=\"tag\">\n",
      "          artificial creativity\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/lovelace/\" rel=\"tag\">\n",
      "          Lovelace\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/mark-o-riedl/\" rel=\"tag\">\n",
      "          Mark O. Riedl\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/the-lovelace-2-0-test-of-artificial-creativity-and-intelligence/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on The Lovelace 2.0 Test of Artificial Creativity and Intelligence\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-123 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers tag-armand-joulin tag-neural-networks tag-piotr-bojanowski tag-recurrent-neural-networks tag-tomas-mikolov\" id=\"post-123\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/alternative-structures-for-character-level-rnns/\">\n",
      "        <img alt=\"Alternative structures for character-level RNNs\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/12/rnn_alt-825x510.png\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/alternative-structures-for-character-level-rnns/\" rel=\"bookmark\">\n",
      "          Alternative structures for character-level RNNs\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         <strong>\n",
      "          Abstract\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         Recurrent neural networks are convenient and efficient models for language modeling. However, when applied on the level of characters instead of words, they suffer from several problems. In order to successfully model long-term dependencies, the hidden representation needs to be large. This in turn implies higher computational costs, which can become prohibitive in practice. We propose two alternative structural modifications to the classical RNN model. The first one consists on conditioning the character level representation on the previous word representation. The other one uses the character history to condition the output probability. We evaluate the performance of the two proposed modifications on challenging, multi-lingual real world data.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Piotr Bojanowski ∗\n",
      "         </strong>\n",
      "         <br/>\n",
      "         INRIA\n",
      "         <br/>\n",
      "         Paris, France\n",
      "         <br/>\n",
      "         piotr.bojanowski@inria.fr\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          Armand Joulin and Tomas Mikolov\n",
      "         </strong>\n",
      "         <br/>\n",
      "         Facebook AI Research\n",
      "         <br/>\n",
      "         New York, NY, USA\n",
      "         <br/>\n",
      "         tmikolov.ajoulin@fb.com\n",
      "        </p>\n",
      "        <p>\n",
      "         <a href=\"http://airesearch.com/wp-content/uploads/2015/12/Alternative_structures_for_character-level_RNNs.pdf\" target=\"_blank\">\n",
      "          Download the full paper here\n",
      "         </a>\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/alternative-structures-for-character-level-rnns/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published\" datetime=\"2015-12-30T16:10:58+00:00\">\n",
      "           December 30, 2015\n",
      "          </time>\n",
      "          <time class=\"updated\" datetime=\"2016-01-26T12:59:40+00:00\">\n",
      "           January 26, 2016\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"tags-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Tags\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/tag/armand-joulin/\" rel=\"tag\">\n",
      "          Armand Joulin\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/neural-networks/\" rel=\"tag\">\n",
      "          Neural Networks\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/piotr-bojanowski/\" rel=\"tag\">\n",
      "          Piotr Bojanowski\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/recurrent-neural-networks/\" rel=\"tag\">\n",
      "          Recurrent Neural Networks\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/tomas-mikolov/\" rel=\"tag\">\n",
      "          Tomas Mikolov\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/alternative-structures-for-character-level-rnns/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on Alternative structures for character-level RNNs\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-114 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-research-papers tag-abdel-rahman-mohamed tag-andrew-senior tag-brian-kingsbury tag-deep-neural-networks tag-dong-yu tag-geoffrey-hinton tag-george-dahl tag-li-deng tag-navdeep-jaitly tag-neural-networks tag-patrick-nguyen tag-speech-recognition tag-tara-sainath tag-vincent-vanhoucke\" id=\"post-114\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speech-recognition/\">\n",
      "        <img alt=\"Deep Neural Networks for Acoustic Modeling in Speech Recognition\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/10/network-825x510.jpg\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speech-recognition/\" rel=\"bookmark\">\n",
      "          Deep Neural Networks for Acoustic Modeling in Speech Recognition\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed- forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output.\n",
      "        </p>\n",
      "        <p>\n",
      "         Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          INTRODUCTION\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         New machine learning algorithms can lead to significant advances in automatic speech recognition. The biggest single advance occured nearly four decades ago with the introduction of the Expectation-Maximization (EM) algorithm for training Hidden Markov Models (HMMs). With the EM algorithm, it became possible to develop speech recognition systems for real world tasks using the richness of Gaussian mixture models (GMM) to represent the relationship between HMM states and the acoustic input. In these systems the acoustic input is typically represented by concatenating Mel Frequency Cepstral Coefficients (MFCCs) or Perceptual Linear Predictive coefficients (PLPs) computed from the raw waveform, and their first- and second-order temporal differences. This non-adaptive but highly- engineered pre-processing of the waveform is designed to discard the large amount of information in waveforms that is considered to be irrelevant for discrimination and to express the remaining information in a form that facilitates discrimination with GMM-HMMs.\n",
      "        </p>\n",
      "        <p>\n",
      "         GMMs have a number of advantages that make them suitable for modeling the probability distributions over vectors of input features that are associated with each state of an HMM. With enough components, they can model probability distributions to any required level of accuracy and they are fairly easy to fit to data using the EM algorithm. A huge amount of research has gone into ways of constraining GMMs to increase their evaluation speed and to optimize the trade-off between their flexibility and the amount of training data available to avoid serious overfitting.\n",
      "        </p>\n",
      "        <p>\n",
      "         The recognition accuracy of a GMM-HMM system can be further improved if it is discriminatively fine-tuned after it has been generatively trained to maximize its probability of generating the observed data, especially if the discriminative objective function used for training is closely related to the error rate on phones, words or sentences[7]. The accuracy can also be improved by augmenting (or concatenating) the input features (e.g., MFCCs) with “tandem” or bottleneck features generated using neural networks. GMMs are so successful that it is difficult for any new method to outperform them for acoustic modeling.\n",
      "        </p>\n",
      "        <p>\n",
      "         Despite all their advantages, GMMs have a serious shortcoming – they are statistically inefficient for modeling data that lie on or near a non-linear manifold in the data space. For example, modeling the set of points that lie very close to the surface of a sphere only requires a few parameters using an appropriate model class, but it requires a very large number of diagonal Gaussians or a fairly large number of full-covariance Gaussians. Speech is produced by modulating a relatively small number of parameters of a dynamical system [10], [11] and this implies that its true underlying structure is much lower-dimensional than is immediately apparent in a window that contains hundreds of coefficients. We believe, therefore, that other types of model may work better than GMMs for acoustic modeling if they can more effectively exploit information embedded in a large window of frames.\n",
      "        </p>\n",
      "        <p>\n",
      "         Artificial neural networks trained by backpropagating error derivatives have the potential to learn much better models of data that lie on or near a non-linear manifold. In fact two decades ago, researchers achieved some success using artificial neural networks with a single layer of non-linear hidden units to predict HMM states from windows of acoustic coefficients. At that time, however, neither the hardware nor the learning algorithms were adequate for training neural networks with many hidden layers on large amounts of data and the performance benefits of using neural networks with a single hidden layer were not sufficiently large to seriously challenge GMMs. As a result, the main practical contribution of neural networks at that time was to provide extra features in tandem or bottleneck systems.\n",
      "        </p>\n",
      "        <p>\n",
      "         Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (DNNs) that contain many layers of non-linear hidden units and a very large output layer. The large output layer is required to accommodate the large number of HMM states that arise when each phone is modelled by a number of different “triphone” HMMs that take into account the phones on either side. Even when many of the states of these triphone HMMs are tied together, there can be thousands of tied states. Using the new learning methods, several different research groups have shown that DNNs can outperform GMMs at acoustic modeling for speech recognition on a variety of datasets including large datasets with large vocabularies.\n",
      "        </p>\n",
      "        <p>\n",
      "         This review paper aims to represent the shared views of research groups at the University of Toronto, Microsoft Research (MSR), Google and IBM Research, who have all had recent successes in using DNNs for acoustic modeling. The paper starts by describing the two-stage training procedure that is used for fitting the DNNs. In the first stage, layers of feature detectors are initialized, one layer at a time, by fitting a stack of generative models, each of which has one layer of latent variables. These generative models are trained without using any information about the HMM states that the acoustic model will need to discriminate. In the second stage, each generative model in the stack is used to initialize one layer of hidden units in a DNN and the whole network is then discriminatively fine-tuned to predict the target HMM states. These targets are obtained by using a baseline GMM-HMM system to produce a forced alignment.\n",
      "        </p>\n",
      "        <p>\n",
      "         In this paper we review exploratory experiments on the TIMIT database that were used to demonstrate the power of this two-stage training procedure for acoustic modeling. The DNNs that worked well on TIMIT were then applied to five different large vocabulary, continuous speech recognition tasks by three different research groups whose results we also summarize. The DNNs worked well on all of these tasks when compared with highly-tuned GMM-HMM systems and on some of the tasks they outperformed the state-of-the-art by a large margin. We also describe some other uses of DNNs for acoustic modeling and some variations on the training procedure.\n",
      "        </p>\n",
      "        <p>\n",
      "         <strong>\n",
      "          View the full PDF publication\n",
      "          <a href=\"http://airesearch.com/wp-content/uploads/2015/10/Deep-Neural-Networks-for-Acoustic-Modeling-in-Speech-Recognition.pdf\" target=\"_blank\">\n",
      "           here\n",
      "          </a>\n",
      "          .\n",
      "         </strong>\n",
      "        </p>\n",
      "        <p>\n",
      "         [Geoffrey Hinton, Li Deng, Dong Yu, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, George Dahl, and Brian Kingsbury]\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speech-recognition/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published\" datetime=\"2015-10-23T09:46:43+00:00\">\n",
      "           October 23, 2015\n",
      "          </time>\n",
      "          <time class=\"updated\" datetime=\"2016-01-26T13:00:10+00:00\">\n",
      "           January 26, 2016\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"tags-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Tags\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/tag/abdel-rahman-mohamed/\" rel=\"tag\">\n",
      "          Abdel-rahman Mohamed\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/andrew-senior/\" rel=\"tag\">\n",
      "          Andrew Senior\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/brian-kingsbury/\" rel=\"tag\">\n",
      "          Brian Kingsbury\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/deep-neural-networks/\" rel=\"tag\">\n",
      "          Deep Neural Networks\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/dong-yu/\" rel=\"tag\">\n",
      "          Dong Yu\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/geoffrey-hinton/\" rel=\"tag\">\n",
      "          Geoffrey Hinton\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/george-dahl/\" rel=\"tag\">\n",
      "          George Dahl\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/li-deng/\" rel=\"tag\">\n",
      "          Li Deng\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/navdeep-jaitly/\" rel=\"tag\">\n",
      "          Navdeep Jaitly\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/neural-networks/\" rel=\"tag\">\n",
      "          Neural Networks\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/patrick-nguyen/\" rel=\"tag\">\n",
      "          Patrick Nguyen\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/speech-recognition/\" rel=\"tag\">\n",
      "          Speech Recognition\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/tara-sainath/\" rel=\"tag\">\n",
      "          Tara Sainath\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/vincent-vanhoucke/\" rel=\"tag\">\n",
      "          Vincent Vanhoucke\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speech-recognition/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on Deep Neural Networks for Acoustic Modeling in Speech Recognition\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "      <article class=\"post-20 post type-post status-publish format-standard has-post-thumbnail hentry category-ai-blog category-ai-research-papers tag-david-chalmers tag-philosophy tag-singularity\" id=\"post-20\">\n",
      "       <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-blog/the-singularity-a-philosophical-analysis/\">\n",
      "        <img alt=\"The Singularity: A Philosophical Analysis\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/09/singularity-825x510.jpg\" width=\"825\"/>\n",
      "       </a>\n",
      "       <header class=\"entry-header\">\n",
      "        <h2 class=\"entry-title\">\n",
      "         <a href=\"http://airesearch.com/ai-blog/the-singularity-a-philosophical-analysis/\" rel=\"bookmark\">\n",
      "          The Singularity: A Philosophical Analysis\n",
      "         </a>\n",
      "        </h2>\n",
      "       </header>\n",
      "       <!-- .entry-header -->\n",
      "       <div class=\"entry-content\">\n",
      "        <p>\n",
      "         <a href=\"http://en.wikipedia.org/wiki/David_Chalmers\" target=\"_blank\">\n",
      "          David Chalmers\n",
      "         </a>\n",
      "         is a leading philosopher of mind, and the first to publish a major philosophy journal article on the singularity:\n",
      "        </p>\n",
      "        <p>\n",
      "         Chalmers, D. (2010). “\n",
      "         <a href=\"http://airesearch.com/wp-content/uploads/2015/09/singularity.pdf\" target=\"_blank\">\n",
      "          The Singularity: A Philosophical Analysis\n",
      "         </a>\n",
      "         .”\n",
      "         <em>\n",
      "          Journal of Consciousness Studies\n",
      "         </em>\n",
      "         17:7-65.\n",
      "        </p>\n",
      "        <p>\n",
      "         Chalmers’ article is a “survey” article in that it doesn’t cover\n",
      "         <em>\n",
      "          any\n",
      "         </em>\n",
      "         arguments in depth, but quickly surveys a large number of positions and arguments in order to give the reader a “lay of the land.” Because of this, Chalmers’ paper is a remarkably broad and clear introduction to the singularity.\n",
      "        </p>\n",
      "        <p>\n",
      "         Singularitarian authors will also be pleased that they can now cite a peer-reviewed article by a leading philosopher of mind who takes the singularity seriously.\n",
      "        </p>\n",
      "        <p>\n",
      "         Below is a\n",
      "         <a href=\"http://en.wikipedia.org/wiki/CliffsNotes\" target=\"_blank\">\n",
      "          CliffsNotes\n",
      "         </a>\n",
      "         of the paper for those who don’t have time to read all 58 pages of it.\n",
      "        </p>\n",
      "        <h4>\n",
      "         The Singularity: Is It Likely?\n",
      "        </h4>\n",
      "        <p>\n",
      "         Chalmers focuses on the “intelligence explosion” kind of singularity, and his first project is to formalise and defend I.J. Good’s 1965 argument. Defining AI as being “of human level intelligence,” AI+ as AI “of greater than human level” and AI++ as “AI of far greater than human level” (super intelligence), Chalmers updates Good’s argument to the following:\n",
      "        </p>\n",
      "        <ol>\n",
      "         <li>\n",
      "          There will be AI (before long, absent defeaters).\n",
      "         </li>\n",
      "         <li>\n",
      "          If there is AI, there will be AI+ (soon after, absent defeaters).\n",
      "         </li>\n",
      "         <li>\n",
      "          If there is AI+, there will be AI++ (soon after, absent defeaters).\n",
      "         </li>\n",
      "         <li>\n",
      "          Therefore, there will be AI++ (before too long, absent defeaters).\n",
      "         </li>\n",
      "        </ol>\n",
      "        <p>\n",
      "         By “defeaters,” Chalmers means global catastrophes like nuclear war or a major asteroid impact. One way to satisfy premise (1) is to achieve AI through brain emulation (Sandberg &amp; Bostrom, 2008). Against this suggestion, Lucas (1961), Dreyfus (1972), and Penrose (1994) argue that human cognition is not the sort of thing that could be emulated. Chalmers (1995; 1996, chapter 9) has responded to these criticisms at length. Briefly, Chalmers notes that even if the brain is not a rule-following algorithmic symbol system, we can still emulate it if it is mechanical. (Some say the brain is not mechanical, but Chalmers dismisses this as being discordant with the evidence.)\n",
      "         <a id=\"more\">\n",
      "         </a>\n",
      "         <br/>\n",
      "         Searle (1980) and Block (1981) argue instead that even\n",
      "         <em>\n",
      "          if\n",
      "         </em>\n",
      "         we can emulate the human brain, it doesn’t follow that the emulation is intelligent or has a mind. Chalmers says we can set these concerns aside by stipulating that when discussing the singularity, AI need only be measured in terms of\n",
      "         <em>\n",
      "          behavior\n",
      "         </em>\n",
      "         . The conclusion that there will be AI++ at least in\n",
      "         <em>\n",
      "          this\n",
      "         </em>\n",
      "         sense would still be massively important.\n",
      "        </p>\n",
      "        <p>\n",
      "         Another consideration in favor of premise (1) is that evolution produced human-level intelligence, so we should be able to build it, too. Perhaps we will even achieve human-level AI by evolving a population of dumber AIs through variation and selection in virtual worlds. We might also achieve human-level AI by direct programming or, more likely, systems of machine learning.\n",
      "        </p>\n",
      "        <p>\n",
      "         Premise (2) is plausible because AI will probably be produced by an extendible method, and so extending that method will yield AI+. Brain emulation\n",
      "         <em>\n",
      "          might\n",
      "         </em>\n",
      "         turn out not to be extendible, but the other methods are. Even if human-level AI is first created by a non-extendible method, this method itself would soon lead to an extendible method, and in turn enable AI+. AI+ could also be achieved by direct brain enhancement.\n",
      "        </p>\n",
      "        <p>\n",
      "         Premise (3) is the amplification argument from Good: an AI+ would be better than we are at designing intelligent machines, and could thus improve its own intelligence. Having done that, it would be even\n",
      "         <em>\n",
      "          better\n",
      "         </em>\n",
      "         at improving its intelligence. And so on, in a rapid explosion of intelligence.\n",
      "        </p>\n",
      "        <p>\n",
      "         In section 3 of his paper, Chalmers argues that there could be an intelligence explosion without there being such a thing as “general intelligence” that could be measured, but I won’t cover that here.\n",
      "        </p>\n",
      "        <p>\n",
      "         In section 4, Chalmers lists several possible obstacles to the singularity.\n",
      "        </p>\n",
      "        <h4>\n",
      "         Constraining AI\n",
      "        </h4>\n",
      "        <p>\n",
      "         Next, Chalmers considers how we might design an AI+ that helps to create a desirable future and not a horrifying one. If we achieve AI+ by extending the method of human brain emulation, the AI+ will at least begin with something like our values. Directly programming friendly values into an AI+ (Yudkowsky, 2004) might also be feasible, though an AI+ arrived at by evolutionary algorithms is worrying.\n",
      "        </p>\n",
      "        <p>\n",
      "         Most of this assumes that values are independent of intelligence, as Hume argued. But if Hume was wrong and Kant was right, then we will be less able to constrain the values of a superintelligent machine, but the more rational the machine is, the better values it will have.\n",
      "        </p>\n",
      "        <p>\n",
      "         Another way to constrain an AI is not internal but external. For example, we could lock it in a virtual world from which it could not escape, and in this way create a\n",
      "         <em>\n",
      "          leakproof singularity\n",
      "         </em>\n",
      "         . But there is a problem. For the AI to be of use to us, some information must leak out of the virtual world for us to observe it. But then, the singularity is not leakproof. And if the AI can communicate us, it could reverse-engineer human psychology from within its virtual world and persuade us to let it out of its box – into the internet, for example.\n",
      "        </p>\n",
      "        <h4>\n",
      "         Our Place in a Post-Singularity World\n",
      "        </h4>\n",
      "        <p>\n",
      "         Chalmers says there are four options for us in a post-singularity world: extinction, isolation, inferiority, and integration.\n",
      "        </p>\n",
      "        <p>\n",
      "         The first option is undesirable. The second option would keep us isolated from the AI, a kind of technological isolationism in which one world is blind to progress in the other. The third option may be infeasible because an AI++ would operate so much faster than us that inferiority is only a blink of time on the way to extinction.\n",
      "        </p>\n",
      "        <p>\n",
      "         For the fourth option to work, we would need to become superintelligent machines ourselves. One path to this mind be\n",
      "         <em>\n",
      "          mind uploading\n",
      "         </em>\n",
      "         , which comes in several varieties and has implications for our notions of consciousness and personal identity that Chalmers discusses but I will not. (Short story: Chalmers prefers gradual uploading, and considers it a form of survival.)\n",
      "        </p>\n",
      "        <h4>\n",
      "         Conclusion\n",
      "        </h4>\n",
      "        <p>\n",
      "         Chalmers concludes:\n",
      "        </p>\n",
      "        <blockquote>\n",
      "         <p>\n",
      "          Will there be a singularity? I think that it is certainly not out of the question, and that the main obstacles are likely to be obstacles of motivation rather than obstacles of capacity.\n",
      "         </p>\n",
      "         <p>\n",
      "          How should we negotiate the singularity? Very carefully, by building appropriate values into machines, and by building the first AI and AI+ systems in virtual worlds.\n",
      "         </p>\n",
      "         <p>\n",
      "          How can we integrate into a post-singularity world? By gradual uploading followed by enhancement if we are still around then, and by reconstructive uploading followed by enhancement if we are not.\n",
      "         </p>\n",
      "        </blockquote>\n",
      "        <h5>\n",
      "         References\n",
      "        </h5>\n",
      "        <p>\n",
      "         Block (1981). “Psychologism and behaviorism.”\n",
      "         <em>\n",
      "          Philosophical Review\n",
      "         </em>\n",
      "         90:5-43.\n",
      "        </p>\n",
      "        <p>\n",
      "         Chalmers (1995). “Minds, machines, and mathematics.”\n",
      "         <em>\n",
      "          Psyche\n",
      "         </em>\n",
      "         2:11-20.\n",
      "        </p>\n",
      "        <p>\n",
      "         Chalmers (1996).\n",
      "         <em>\n",
      "          The Conscious Mind\n",
      "         </em>\n",
      "         . Oxford University Press.\n",
      "        </p>\n",
      "        <p>\n",
      "         Dreyfus (1972).\n",
      "         <em>\n",
      "          What Computers Can’t Do\n",
      "         </em>\n",
      "         . Harper &amp; Row.\n",
      "        </p>\n",
      "        <p>\n",
      "         Lucas (1961). “Minds, machines, and Godel.”\n",
      "         <em>\n",
      "          Philosophy\n",
      "         </em>\n",
      "         36:112-27.\n",
      "        </p>\n",
      "        <p>\n",
      "         Penrose (1994).\n",
      "         <em>\n",
      "          Shadows of the Mind\n",
      "         </em>\n",
      "         . Oxford University Press.\n",
      "        </p>\n",
      "        <p>\n",
      "         Sandberg &amp; Bostrom (2008). “Whole brain emulation: A roadmap.” Technical report 2008-3, Future for Humanity Institute, Oxford University.\n",
      "        </p>\n",
      "        <p>\n",
      "         Searle (1980). “Minds, brains, and programs.”\n",
      "         <em>\n",
      "          Behavioral and Brain Sciences\n",
      "         </em>\n",
      "         3:417-57.\n",
      "        </p>\n",
      "        <p>\n",
      "         Yudkowsky (2004). “\n",
      "         <a href=\"http://intelligence.org/upload/CEV.html\" target=\"_blank\">\n",
      "          Coherent Extrapolated Volition\n",
      "         </a>\n",
      "         .”\n",
      "        </p>\n",
      "       </div>\n",
      "       <!-- .entry-content -->\n",
      "       <footer class=\"entry-footer\">\n",
      "        <span class=\"posted-on\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Posted on\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/ai-blog/the-singularity-a-philosophical-analysis/\" rel=\"bookmark\">\n",
      "          <time class=\"entry-date published\" datetime=\"2015-09-16T19:19:19+00:00\">\n",
      "           September 16, 2015\n",
      "          </time>\n",
      "          <time class=\"updated\" datetime=\"2016-01-26T13:03:03+00:00\">\n",
      "           January 26, 2016\n",
      "          </time>\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"cat-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Categories\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/category/ai-blog/\" rel=\"category tag\">\n",
      "          Blog Articles\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/category/ai-research-papers/\" rel=\"category tag\">\n",
      "          Research Papers\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"tags-links\">\n",
      "         <span class=\"screen-reader-text\">\n",
      "          Tags\n",
      "         </span>\n",
      "         <a href=\"http://airesearch.com/tag/david-chalmers/\" rel=\"tag\">\n",
      "          David Chalmers\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/philosophy/\" rel=\"tag\">\n",
      "          Philosophy\n",
      "         </a>\n",
      "         ,\n",
      "         <a href=\"http://airesearch.com/tag/singularity/\" rel=\"tag\">\n",
      "          Singularity\n",
      "         </a>\n",
      "        </span>\n",
      "        <span class=\"comments-link\">\n",
      "         <a href=\"http://airesearch.com/ai-blog/the-singularity-a-philosophical-analysis/#respond\">\n",
      "          Leave a comment\n",
      "          <span class=\"screen-reader-text\">\n",
      "           on The Singularity: A Philosophical Analysis\n",
      "          </span>\n",
      "         </a>\n",
      "        </span>\n",
      "       </footer>\n",
      "       <!-- .entry-footer -->\n",
      "      </article>\n",
      "      <!-- #post-## -->\n",
      "     </main>\n",
      "     <!-- .site-main -->\n",
      "    </section>\n",
      "    <!-- .content-area -->\n",
      "   </div>\n",
      "   <!-- .site-content -->\n",
      "   <footer class=\"site-footer\" id=\"colophon\" role=\"contentinfo\">\n",
      "    <div class=\"site-info\">\n",
      "     <script async=\"\" src=\"//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\">\n",
      "     </script>\n",
      "     <!-- AI Research -->\n",
      "     <ins class=\"adsbygoogle\" data-ad-client=\"ca-pub-2152707927434184\" data-ad-format=\"auto\" data-ad-slot=\"6946804555\" style=\"display:block\">\n",
      "     </ins>\n",
      "     <script>\n",
      "      (adsbygoogle = window.adsbygoogle || []).push({});\n",
      "     </script>\n",
      "     <br/>\n",
      "     <br/>\n",
      "     <a href=\"http://airesearch.com\">\n",
      "      AI Research\n",
      "     </a>\n",
      "     v2 |\n",
      "     <a href=\"mailto:info@airesearch.com\">\n",
      "      info@airesearch.com\n",
      "     </a>\n",
      "    </div>\n",
      "    <!-- .site-info -->\n",
      "   </footer>\n",
      "   <!-- .site-footer -->\n",
      "  </div>\n",
      "  <!-- .site -->\n",
      "  <script src=\"http://airesearch.com/wp-content/themes/twentyfifteen/js/skip-link-focus-fix.js?ver=20141010\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script type=\"text/javascript\">\n",
      "   /* <![CDATA[ */\n",
      "var screenReaderText = {\"expand\":\"<span class=\\\"screen-reader-text\\\">expand child menu<\\/span>\",\"collapse\":\"<span class=\\\"screen-reader-text\\\">collapse child menu<\\/span>\"};\n",
      "/* ]]> */\n",
      "  </script>\n",
      "  <script src=\"http://airesearch.com/wp-content/themes/twentyfifteen/js/functions.js?ver=20150330\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script src=\"http://airesearch.com/wp-includes/js/wp-embed.min.js?ver=4.4.1\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script>\n",
      "   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\r\n",
      "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\r\n",
      "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\r\n",
      "  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\r\n",
      "\r\n",
      "  ga('create', 'UA-71875859-1', 'auto');\r\n",
      "  ga('send', 'pageview');\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "soup = BeautifulSoup(requests.get(\"http://airesearch.com/category/ai-research-papers/\").text, 'lxml')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained: XLNet – Generalized Autoregressive Pretraining for Language Understanding\n"
     ]
    }
   ],
   "source": [
    "Title = soup.find('h2', class_='entry-title')\n",
    "print(Title.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained: XLNet – Generalized Autoregressive Pretraining for Language Understanding\n",
      "Overcoming Priors for Visual Question Answering\n",
      "The importance of single directions for generalization\n",
      "SingularityNET: A decentralized, open market and inter-network for AIs\n",
      "WaveNet: A Generative Model for Raw Audio\n",
      "Facebook AI Research – Machine Intelligence Roadmap\n",
      "The Lovelace 2.0 Test of Artificial Creativity and Intelligence\n",
      "Alternative structures for character-level RNNs\n",
      "Deep Neural Networks for Acoustic Modeling in Speech Recognition\n",
      "The Singularity: A Philosophical Analysis\n"
     ]
    }
   ],
   "source": [
    "for Title in soup.find_all('h2', class_='entry-title'):\n",
    "    print(Title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/explained-xlnet-generalized-autoregressive-pretraining-for-language-understanding/\">\n",
      "<img alt=\"Explained: XLNet – Generalized Autoregressive Pretraining for Language Understanding\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2019/07/xlnet-825x510.png\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/overcoming-priors-for-visual-question-answering/\">\n",
      "<img alt=\"Overcoming Priors for Visual Question Answering\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/12/facebook_ai_research-825x510.png\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/the-importance-of-single-directions-for-generalization/\">\n",
      "<img alt=\"The importance of single directions for generalization\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2016/10/deepmind-825x510.png\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/singularitynet-a-decentralized-open-market-and-inter-network-for-ais/\">\n",
      "<img alt=\"SingularityNET: A decentralized, open market and inter-network for AIs\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2017/12/singularity-1-825x510.jpg\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/wavenet-a-generative-model-for-raw-audio/\">\n",
      "<img alt=\"WaveNet: A Generative Model for Raw Audio\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2016/10/deepmind-825x510.png\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/facebook-ai-research-machine-intelligence-roadmap/\">\n",
      "<img alt=\"Facebook AI Research – Machine Intelligence Roadmap\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/12/facebook_ai_research-825x510.png\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/the-lovelace-2-0-test-of-artificial-creativity-and-intelligence/\">\n",
      "<img alt=\"The Lovelace 2.0 Test of Artificial Creativity and Intelligence\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2016/01/ai_art-825x510.jpg\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/alternative-structures-for-character-level-rnns/\">\n",
      "<img alt=\"Alternative structures for character-level RNNs\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/12/rnn_alt-825x510.png\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speech-recognition/\">\n",
      "<img alt=\"Deep Neural Networks for Acoustic Modeling in Speech Recognition\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/10/network-825x510.jpg\" width=\"825\"/> </a>, <a aria-hidden=\"true\" class=\"post-thumbnail\" href=\"http://airesearch.com/ai-blog/the-singularity-a-philosophical-analysis/\">\n",
      "<img alt=\"The Singularity: A Philosophical Analysis\" class=\"attachment-post-thumbnail size-post-thumbnail wp-post-image\" height=\"510\" src=\"http://airesearch.com/wp-content/uploads/2015/09/singularity-825x510.jpg\" width=\"825\"/> </a>]\n"
     ]
    }
   ],
   "source": [
    "url = soup.find_all('a', class_='post-thumbnail')\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://airesearch.com/ai-research-papers/explained-xlnet-generalized-autoregressive-pretraining-for-language-understanding/\n",
      "http://airesearch.com/ai-research-papers/overcoming-priors-for-visual-question-answering/\n",
      "http://airesearch.com/ai-research-papers/the-importance-of-single-directions-for-generalization/\n",
      "http://airesearch.com/ai-research-papers/singularitynet-a-decentralized-open-market-and-inter-network-for-ais/\n",
      "http://airesearch.com/ai-research-papers/wavenet-a-generative-model-for-raw-audio/\n",
      "http://airesearch.com/ai-research-papers/facebook-ai-research-machine-intelligence-roadmap/\n",
      "http://airesearch.com/ai-research-papers/the-lovelace-2-0-test-of-artificial-creativity-and-intelligence/\n",
      "http://airesearch.com/ai-research-papers/alternative-structures-for-character-level-rnns/\n",
      "http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speech-recognition/\n",
      "http://airesearch.com/ai-blog/the-singularity-a-philosophical-analysis/\n"
     ]
    }
   ],
   "source": [
    "for url in soup.find_all('a', class_='post-thumbnail'):\n",
    "    print(url['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"entry-content\">\n",
      "<p>Carnegie Mellon and Google’s Brain outfit have tried to undo some of the techniques of Google’s BERT machine learning model for natural language processing.</p>\n",
      "<p>They propose a new approach called “XLNet.” Built on top of the popular “Transformer” A.I. for language, it may be a more straightforward way to examine how language works.</p>\n",
      "<p>XLNet is an exciting development in NLP, not only because of its results but because it shows us that there is still room to improve upon for transfer learning in NLP.</p>\n",
      "<p>Machine Learning Explained’s article, <a href=\"https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\" target=\"_blank\">Paper Dissected: “XLNet: Generalized Autoregressive Pretraining for Language Understanding” Explained</a>, offers a clear summary of arguably one of 2019’s most important developments in Natural Language Processing.</p>\n",
      "<p><strong>Further Readings</strong></p>\n",
      "<p><a href=\"https://arxiv.org/pdf/1906.08237.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">The original paper</a></p>\n",
      "<p><a href=\"http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/\" rel=\"noopener noreferrer\" target=\"_blank\">Blog post on the Transformer</a></p>\n",
      "<p><a href=\"http://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/\" rel=\"noopener noreferrer\" target=\"_blank\">Blog post on ELMo</a></p>\n",
      "<p><a href=\"http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/\" rel=\"noopener noreferrer\" target=\"_blank\">Blog post on BERT</a></p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<h3>Abstract</h3>\n",
      "<p>A number of studies have found that today’s Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from ‘cheating’ by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model – Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.</p>\n",
      "<p><strong>Aishwarya Agrawal</strong>, <strong><a class=\"author-link\" href=\"https://research.fb.com/people/batra-dhruv/\">Dhruv Batra</a></strong>, <strong><a class=\"author-link\" href=\"https://research.fb.com/people/parikh-devi/\">Devi Parikh</a></strong>, <strong>Aniruddha Kembhavi<br/>\n",
      "</strong>[Facebook AI Research]</p>\n",
      "<p><a href=\"https://research.fb.com/wp-content/uploads/2018/05/done28099t-just-assume-look-and-answer-overcoming-priors-for-visual-question-answering.pdf?\" target=\"_blank\">Download the full paper here</a></p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<p><strong>ABSTRACT</strong></p>\n",
      "<p>Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network’s reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.</p>\n",
      "<p><strong>Ari S. Morcos</strong>, <strong>David G.T. Barrett</strong>, <strong>Neil C. Rabinowitz</strong>, &amp; <strong>Matthew Botvinick<br/>\n",
      "@DeepMind</strong></p>\n",
      "<p><a href=\"ttps://arxiv.org/pdf/1803.06959.pdf\" target=\"_blank\">Download the full paper here</a></p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<p><strong>ABSTRACT</strong></p>\n",
      "<p>The value and power of Artificial Intelligence is growing dramatically every year, and will soon dominate the internet – and the economy as a whole. However, AI tools today are fragmented by a closed development environment; most are developed by one company to perform one task, and there is no way to plug two tools together. SingularityNET aims to become the key protocol for networking AI and machine learning tools to form a coordinated Artificial General Intelligence. SingularityNET is an open-source protocol and collection of smart contracts for a decentralized market of coordinated AI services. Within this framework, the benefits of AI become a global commons infrastructure for the benefit of all; anyone can access AI tech or become a stakeholder in its development. Anyone can add an AI/machine learning service to SingularityNET for use by the network, and receive network payment tokens in exchange. SingularityNET is backed by the SingularityNET Foundation, which operates on a belief that the benefits of AI should not be dominated by any small set of powerful institutions, but shared by all. A key goal of SingularityNET is to ensure the technology is benevolent according to human standards, and the network is designed to incentivize and reward beneficial players.</p>\n",
      "<p><strong>Ben Goertzel</strong></p>\n",
      "<p><a href=\"http://airesearch.com/wp-content/uploads/2017/12/SingularityNET.pdf\" target=\"_blank\">Download the full paper here</a></p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<p><strong>ABSTRACT</strong></p>\n",
      "<p>This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.</p>\n",
      "<p><strong>Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu<br/>\n",
      "</strong><em>{avdnoord, sedielem, heigazen, simonyan, vinyals, gravesa, nalk, andrewsenior, korayk}@google.com</em><br/>\n",
      "<em>Google DeepMind</em></p>\n",
      "<p><a href=\"https://arxiv.org/pdf/1609.03499.pdf\" target=\"_blank\">Download the full paper here</a></p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<p><strong>Abstract</strong></p>\n",
      "<p>The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.</p>\n",
      "<p><strong>Tomas Mikolov, Armand Joulin, Marco Baroni</strong><br/>\n",
      "<em>Facebook AI Research</em></p>\n",
      "<p><strong>1 Introduction</strong></p>\n",
      "<p>A machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavour, from performing menial jobs for us to helping the advancement of basic and applied research. Given the current availability of powerful hardware and large amounts of machine-readable data, as well as the widespread interest in sophisticated machine learning methods, the times should be ripe for the development of intelligent machines.</p>\n",
      "<p>We think that one fundamental reasons for this is that, since “solving AI” at once seems too complex a task to be pursued all at once, the computational community has preferred to focus, in the last decades, on solving relatively narrow empirical problems that are important for specific applications, but do not address the overarching goal of developing general-purpose intelligent machines. </p>\n",
      "<p>In this article, we propose an alternative approach: we first define the general characteristics we think intelligent machines should possess, and then we present a concrete roadmap to develop them in realistic, small steps, that are however incrementally structured in such a way that, jointly, they should lead us close to the ultimate goal of implementing a powerful AI. We realise that our vision of artificial intelligence and how to create it is just one among many. We focus here on a plan that, we hope, will lead to genuine progress, without by this implying that there are not other valid approaches to the task.</p>\n",
      "<p>The article is structured as follows. In Section 2 we indicate the two fundamental characteristics that we consider crucial for developing intelligence– at least the sort of intelligence we are interested in–namely communication and learning. Our goal is to build a machine that can learn new concepts through communication at a similar rate as a human with similar prior knowledge. That is, if one can easily learn how subtraction works after mastering addition, the intelligent machine, after grasping the concept of addition, should not find it difficult to learn subtraction as well.</p>\n",
      "<p>Since, as we said, achieving the long-term goal of building an intelligent machine equipped with the desired features at once seems too difficult, we need to define intermediate targets that can lead us in the right direction. We specify such targets in terms of simplified but self-contained versions of the final machine we want to develop. Our plan is to “educate” the target machine like a child: At any time in its development, the target machine should act like a stand-alone intelligent system, albeit one that will be initially very limited in what it can do. The bulk of our proposal (Section 3) thus consists in the plan for an interactive learning environment fostering the incremental development of progressively more intelligent behaviour. </p>\n",
      "<p>Section 4 briefly discusses some of the algorithmic capabilities we think a machine should possess in order to profitably exploit the learning environment. Finally, Section 5 situates our proposal in the broader context of past and current attempts to develop intelligent machines.</p>\n",
      "<p><a href=\"http://airesearch.com/wp-content/uploads/2016/01/A_Roadmap_towards_Machine_Intelligence.pdf\" target=\"_blank\">Download the full paper here</a></p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<p>The Lovelace 2.0 Test asks whether a computer can create an artefact – a poem, story, painting or architectural design – that expert and unbiased observers would conclude was designed by a human. </p>\n",
      "<p>Prof. Mark Riedl proposes the concept of artificial creativity, akin to artificial intelligence. This could be tested he says, using an alternative to the Turing Test, the AI benchmark that asserts that if a computer system can fool a human being into thinking it is human itself, then it can be said to be truly intelligent. </p>\n",
      "<p>You can read Prof. Riedl’s paper below: </p>\n",
      "<p><strong>Abstract</strong></p>\n",
      "<p>Observing that the creation of certain types of artistic artefacts necessitate intelligence, we present the Lovelace 2.0 Test of creativity as an alternative to the Turing Test as a means of determining whether an agent is intelligent.</p>\n",
      "<p>The Lovelace 2.0 Test builds off prior tests of creativity and additionally provides a means of directly comparing the relative intelligence of different agents.</p>\n",
      "<p><strong>Mark O. Riedl</strong><br/>\n",
      "School of Interactive Computing; Georgia Institute of Technology<br/>\n",
      "riedl@cc.gatech.edu</p>\n",
      "<p><a href=\"http://airesearch.com/wp-content/uploads/2016/01/1410.6142v1.pdf\" target=\"_blank\">Download the full paper here</a></p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<p><strong>Abstract</strong></p>\n",
      "<p>Recurrent neural networks are convenient and efficient models for language modeling. However, when applied on the level of characters instead of words, they suffer from several problems. In order to successfully model long-term dependencies, the hidden representation needs to be large. This in turn implies higher computational costs, which can become prohibitive in practice. We propose two alternative structural modifications to the classical RNN model. The first one consists on conditioning the character level representation on the previous word representation. The other one uses the character history to condition the output probability. We evaluate the performance of the two proposed modifications on challenging, multi-lingual real world data.</p>\n",
      "<p><strong>Piotr Bojanowski ∗</strong><br/>\n",
      "INRIA<br/>\n",
      "Paris, France<br/>\n",
      "piotr.bojanowski@inria.fr</p>\n",
      "<p><strong>Armand Joulin and Tomas Mikolov</strong><br/>\n",
      "Facebook AI Research<br/>\n",
      "New York, NY, USA<br/>\n",
      "tmikolov.ajoulin@fb.com</p>\n",
      "<p><a href=\"http://airesearch.com/wp-content/uploads/2015/12/Alternative_structures_for_character-level_RNNs.pdf\" target=\"_blank\">Download the full paper here</a></p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<p>Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed- forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output.</p>\n",
      "<p>Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.</p>\n",
      "<p><strong>INTRODUCTION</strong></p>\n",
      "<p>New machine learning algorithms can lead to significant advances in automatic speech recognition. The biggest single advance occured nearly four decades ago with the introduction of the Expectation-Maximization (EM) algorithm for training Hidden Markov Models (HMMs). With the EM algorithm, it became possible to develop speech recognition systems for real world tasks using the richness of Gaussian mixture models (GMM) to represent the relationship between HMM states and the acoustic input. In these systems the acoustic input is typically represented by concatenating Mel Frequency Cepstral Coefficients (MFCCs) or Perceptual Linear Predictive coefficients (PLPs) computed from the raw waveform, and their first- and second-order temporal differences. This non-adaptive but highly- engineered pre-processing of the waveform is designed to discard the large amount of information in waveforms that is considered to be irrelevant for discrimination and to express the remaining information in a form that facilitates discrimination with GMM-HMMs.</p>\n",
      "<p>GMMs have a number of advantages that make them suitable for modeling the probability distributions over vectors of input features that are associated with each state of an HMM. With enough components, they can model probability distributions to any required level of accuracy and they are fairly easy to fit to data using the EM algorithm. A huge amount of research has gone into ways of constraining GMMs to increase their evaluation speed and to optimize the trade-off between their flexibility and the amount of training data available to avoid serious overfitting.</p>\n",
      "<p>The recognition accuracy of a GMM-HMM system can be further improved if it is discriminatively fine-tuned after it has been generatively trained to maximize its probability of generating the observed data, especially if the discriminative objective function used for training is closely related to the error rate on phones, words or sentences[7]. The accuracy can also be improved by augmenting (or concatenating) the input features (e.g., MFCCs) with “tandem” or bottleneck features generated using neural networks. GMMs are so successful that it is difficult for any new method to outperform them for acoustic modeling.</p>\n",
      "<p>Despite all their advantages, GMMs have a serious shortcoming – they are statistically inefficient for modeling data that lie on or near a non-linear manifold in the data space. For example, modeling the set of points that lie very close to the surface of a sphere only requires a few parameters using an appropriate model class, but it requires a very large number of diagonal Gaussians or a fairly large number of full-covariance Gaussians. Speech is produced by modulating a relatively small number of parameters of a dynamical system [10], [11] and this implies that its true underlying structure is much lower-dimensional than is immediately apparent in a window that contains hundreds of coefficients. We believe, therefore, that other types of model may work better than GMMs for acoustic modeling if they can more effectively exploit information embedded in a large window of frames.</p>\n",
      "<p>Artificial neural networks trained by backpropagating error derivatives have the potential to learn much better models of data that lie on or near a non-linear manifold. In fact two decades ago, researchers achieved some success using artificial neural networks with a single layer of non-linear hidden units to predict HMM states from windows of acoustic coefficients. At that time, however, neither the hardware nor the learning algorithms were adequate for training neural networks with many hidden layers on large amounts of data and the performance benefits of using neural networks with a single hidden layer were not sufficiently large to seriously challenge GMMs. As a result, the main practical contribution of neural networks at that time was to provide extra features in tandem or bottleneck systems.</p>\n",
      "<p>Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (DNNs) that contain many layers of non-linear hidden units and a very large output layer. The large output layer is required to accommodate the large number of HMM states that arise when each phone is modelled by a number of different “triphone” HMMs that take into account the phones on either side. Even when many of the states of these triphone HMMs are tied together, there can be thousands of tied states. Using the new learning methods, several different research groups have shown that DNNs can outperform GMMs at acoustic modeling for speech recognition on a variety of datasets including large datasets with large vocabularies.</p>\n",
      "<p>This review paper aims to represent the shared views of research groups at the University of Toronto, Microsoft Research (MSR), Google and IBM Research, who have all had recent successes in using DNNs for acoustic modeling. The paper starts by describing the two-stage training procedure that is used for fitting the DNNs. In the first stage, layers of feature detectors are initialized, one layer at a time, by fitting a stack of generative models, each of which has one layer of latent variables. These generative models are trained without using any information about the HMM states that the acoustic model will need to discriminate. In the second stage, each generative model in the stack is used to initialize one layer of hidden units in a DNN and the whole network is then discriminatively fine-tuned to predict the target HMM states. These targets are obtained by using a baseline GMM-HMM system to produce a forced alignment.</p>\n",
      "<p>In this paper we review exploratory experiments on the TIMIT database that were used to demonstrate the power of this two-stage training procedure for acoustic modeling. The DNNs that worked well on TIMIT were then applied to five different large vocabulary, continuous speech recognition tasks by three different research groups whose results we also summarize. The DNNs worked well on all of these tasks when compared with highly-tuned GMM-HMM systems and on some of the tasks they outperformed the state-of-the-art by a large margin. We also describe some other uses of DNNs for acoustic modeling and some variations on the training procedure.</p>\n",
      "<p><strong>View the full PDF publication <a href=\"http://airesearch.com/wp-content/uploads/2015/10/Deep-Neural-Networks-for-Acoustic-Modeling-in-Speech-Recognition.pdf\" target=\"_blank\">here</a>.</strong></p>\n",
      "<p>[Geoffrey Hinton, Li Deng, Dong Yu, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, George Dahl, and Brian Kingsbury]</p>\n",
      "</div>, <div class=\"entry-content\">\n",
      "<p><a href=\"http://en.wikipedia.org/wiki/David_Chalmers\" target=\"_blank\">David Chalmers</a> is a leading philosopher of mind, and the first to publish a major philosophy journal article on the singularity:</p>\n",
      "<p>Chalmers, D. (2010). “<a href=\"http://airesearch.com/wp-content/uploads/2015/09/singularity.pdf\" target=\"_blank\">The Singularity: A Philosophical Analysis</a>.” <em>Journal of Consciousness Studies</em> 17:7-65.</p>\n",
      "<p>Chalmers’ article is a “survey” article in that it doesn’t cover <em>any</em> arguments in depth, but quickly surveys a large number of positions and arguments in order to give the reader a “lay of the land.” Because of this, Chalmers’ paper is a remarkably broad and clear introduction to the singularity.</p>\n",
      "<p>Singularitarian authors will also be pleased that they can now cite a peer-reviewed article by a leading philosopher of mind who takes the singularity seriously.</p>\n",
      "<p>Below is a <a href=\"http://en.wikipedia.org/wiki/CliffsNotes\" target=\"_blank\">CliffsNotes</a> of the paper for those who don’t have time to read all 58 pages of it.</p>\n",
      "<h4>The Singularity: Is It Likely?</h4>\n",
      "<p>Chalmers focuses on the “intelligence explosion” kind of singularity, and his first project is to formalise and defend I.J. Good’s 1965 argument. Defining AI as being “of human level intelligence,” AI+ as AI “of greater than human level” and AI++ as “AI of far greater than human level” (super intelligence), Chalmers updates Good’s argument to the following:</p>\n",
      "<ol>\n",
      "<li>There will be AI (before long, absent defeaters).</li>\n",
      "<li>If there is AI, there will be AI+ (soon after, absent defeaters).</li>\n",
      "<li>If there is AI+, there will be AI++ (soon after, absent defeaters).</li>\n",
      "<li>Therefore, there will be AI++ (before too long, absent defeaters).</li>\n",
      "</ol>\n",
      "<p>By “defeaters,” Chalmers means global catastrophes like nuclear war or a major asteroid impact. One way to satisfy premise (1) is to achieve AI through brain emulation (Sandberg &amp; Bostrom, 2008). Against this suggestion, Lucas (1961), Dreyfus (1972), and Penrose (1994) argue that human cognition is not the sort of thing that could be emulated. Chalmers (1995; 1996, chapter 9) has responded to these criticisms at length. Briefly, Chalmers notes that even if the brain is not a rule-following algorithmic symbol system, we can still emulate it if it is mechanical. (Some say the brain is not mechanical, but Chalmers dismisses this as being discordant with the evidence.)<a id=\"more\"></a><br/>\n",
      "Searle (1980) and Block (1981) argue instead that even <em>if</em> we can emulate the human brain, it doesn’t follow that the emulation is intelligent or has a mind. Chalmers says we can set these concerns aside by stipulating that when discussing the singularity, AI need only be measured in terms of <em>behavior</em>. The conclusion that there will be AI++ at least in <em>this</em> sense would still be massively important.</p>\n",
      "<p>Another consideration in favor of premise (1) is that evolution produced human-level intelligence, so we should be able to build it, too. Perhaps we will even achieve human-level AI by evolving a population of dumber AIs through variation and selection in virtual worlds. We might also achieve human-level AI by direct programming or, more likely, systems of machine learning.</p>\n",
      "<p>Premise (2) is plausible because AI will probably be produced by an extendible method, and so extending that method will yield AI+. Brain emulation <em>might </em>turn out not to be extendible, but the other methods are. Even if human-level AI is first created by a non-extendible method, this method itself would soon lead to an extendible method, and in turn enable AI+. AI+ could also be achieved by direct brain enhancement.</p>\n",
      "<p>Premise (3) is the amplification argument from Good: an AI+ would be better than we are at designing intelligent machines, and could thus improve its own intelligence. Having done that, it would be even <em>better</em> at improving its intelligence. And so on, in a rapid explosion of intelligence.</p>\n",
      "<p>In section 3 of his paper, Chalmers argues that there could be an intelligence explosion without there being such a thing as “general intelligence” that could be measured, but I won’t cover that here.</p>\n",
      "<p>In section 4, Chalmers lists several possible obstacles to the singularity.</p>\n",
      "<h4>Constraining AI</h4>\n",
      "<p>Next, Chalmers considers how we might design an AI+ that helps to create a desirable future and not a horrifying one. If we achieve AI+ by extending the method of human brain emulation, the AI+ will at least begin with something like our values. Directly programming friendly values into an AI+ (Yudkowsky, 2004) might also be feasible, though an AI+ arrived at by evolutionary algorithms is worrying.</p>\n",
      "<p>Most of this assumes that values are independent of intelligence, as Hume argued. But if Hume was wrong and Kant was right, then we will be less able to constrain the values of a superintelligent machine, but the more rational the machine is, the better values it will have.</p>\n",
      "<p>Another way to constrain an AI is not internal but external. For example, we could lock it in a virtual world from which it could not escape, and in this way create a <em>leakproof singularity</em>. But there is a problem. For the AI to be of use to us, some information must leak out of the virtual world for us to observe it. But then, the singularity is not leakproof. And if the AI can communicate us, it could reverse-engineer human psychology from within its virtual world and persuade us to let it out of its box – into the internet, for example.</p>\n",
      "<h4>Our Place in a Post-Singularity World</h4>\n",
      "<p>Chalmers says there are four options for us in a post-singularity world: extinction, isolation, inferiority, and integration.</p>\n",
      "<p>The first option is undesirable. The second option would keep us isolated from the AI, a kind of technological isolationism in which one world is blind to progress in the other. The third option may be infeasible because an AI++ would operate so much faster than us that inferiority is only a blink of time on the way to extinction.</p>\n",
      "<p>For the fourth option to work, we would need to become superintelligent machines ourselves. One path to this mind be<em>mind uploading</em>, which comes in several varieties and has implications for our notions of consciousness and personal identity that Chalmers discusses but I will not. (Short story: Chalmers prefers gradual uploading, and considers it a form of survival.)</p>\n",
      "<h4>Conclusion</h4>\n",
      "<p>Chalmers concludes:</p>\n",
      "<blockquote><p>Will there be a singularity? I think that it is certainly not out of the question, and that the main obstacles are likely to be obstacles of motivation rather than obstacles of capacity.</p>\n",
      "<p>How should we negotiate the singularity? Very carefully, by building appropriate values into machines, and by building the first AI and AI+ systems in virtual worlds.</p>\n",
      "<p>How can we integrate into a post-singularity world? By gradual uploading followed by enhancement if we are still around then, and by reconstructive uploading followed by enhancement if we are not.</p></blockquote>\n",
      "<h5>References</h5>\n",
      "<p>Block (1981). “Psychologism and behaviorism.” <em>Philosophical Review</em> 90:5-43.</p>\n",
      "<p>Chalmers (1995). “Minds, machines, and mathematics.” <em>Psyche</em> 2:11-20.</p>\n",
      "<p>Chalmers (1996). <em>The Conscious Mind</em>. Oxford University Press.</p>\n",
      "<p>Dreyfus (1972). <em>What Computers Can’t Do</em>. Harper &amp; Row.</p>\n",
      "<p>Lucas (1961). “Minds, machines, and Godel.” <em>Philosophy </em>36:112-27.</p>\n",
      "<p>Penrose (1994). <em>Shadows of the Mind</em>. Oxford University Press.</p>\n",
      "<p>Sandberg &amp; Bostrom (2008). “Whole brain emulation: A roadmap.” Technical report 2008-3, Future for Humanity Institute, Oxford University.</p>\n",
      "<p>Searle (1980). “Minds, brains, and programs.” <em>Behavioral and Brain Sciences</em> 3:417-57.</p>\n",
      "<p>Yudkowsky (2004). “<a href=\"http://intelligence.org/upload/CEV.html\" target=\"_blank\">Coherent Extrapolated Volition</a>.”</p>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "abstract = soup.find_all('div', class_='entry-content')\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carnegie Mellon and Google’s Brain outfit have tried to undo some of the techniques of Google’s BERT machine learning model for natural language processing.\n",
      "They propose a new approach called “XLNet.” Built on top of the popular “Transformer” A.I. for language, it may be a more straightforward way to examine how language works.\n",
      "XLNet is an exciting development in NLP, not only because of its results but because it shows us that there is still room to improve upon for transfer learning in NLP.\n",
      "Machine Learning Explained’s article, Paper Dissected: “XLNet: Generalized Autoregressive Pretraining for Language Understanding” Explained, offers a clear summary of arguably one of 2019’s most important developments in Natural Language Processing.\n",
      "Further Readings\n",
      "The original paper\n",
      "Blog post on the Transformer\n",
      "Blog post on ELMo\n",
      "Blog post on BERT\n",
      "\n",
      "\n",
      "Abstract\n",
      "A number of studies have found that today’s Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from ‘cheating’ by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model – Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.\n",
      "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi\n",
      "[Facebook AI Research]\n",
      "Download the full paper here\n",
      "\n",
      "\n",
      "ABSTRACT\n",
      "Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network’s reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.\n",
      "Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, & Matthew Botvinick\n",
      "@DeepMind\n",
      "Download the full paper here\n",
      "\n",
      "\n",
      "ABSTRACT\n",
      "The value and power of Artificial Intelligence is growing dramatically every year, and will soon dominate the internet – and the economy as a whole. However, AI tools today are fragmented by a closed development environment; most are developed by one company to perform one task, and there is no way to plug two tools together. SingularityNET aims to become the key protocol for networking AI and machine learning tools to form a coordinated Artificial General Intelligence. SingularityNET is an open-source protocol and collection of smart contracts for a decentralized market of coordinated AI services. Within this framework, the benefits of AI become a global commons infrastructure for the benefit of all; anyone can access AI tech or become a stakeholder in its development. Anyone can add an AI/machine learning service to SingularityNET for use by the network, and receive network payment tokens in exchange. SingularityNET is backed by the SingularityNET Foundation, which operates on a belief that the benefits of AI should not be dominated by any small set of powerful institutions, but shared by all. A key goal of SingularityNET is to ensure the technology is benevolent according to human standards, and the network is designed to incentivize and reward beneficial players.\n",
      "Ben Goertzel\n",
      "Download the full paper here\n",
      "\n",
      "\n",
      "ABSTRACT\n",
      "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.\n",
      "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu\n",
      "{avdnoord, sedielem, heigazen, simonyan, vinyals, gravesa, nalk, andrewsenior, korayk}@google.com\n",
      "Google DeepMind\n",
      "Download the full paper here\n",
      "\n",
      "\n",
      "Abstract\n",
      "The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.\n",
      "Tomas Mikolov, Armand Joulin, Marco Baroni\n",
      "Facebook AI Research\n",
      "1 Introduction\n",
      "A machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavour, from performing menial jobs for us to helping the advancement of basic and applied research. Given the current availability of powerful hardware and large amounts of machine-readable data, as well as the widespread interest in sophisticated machine learning methods, the times should be ripe for the development of intelligent machines.\n",
      "We think that one fundamental reasons for this is that, since “solving AI” at once seems too complex a task to be pursued all at once, the computational community has preferred to focus, in the last decades, on solving relatively narrow empirical problems that are important for specific applications, but do not address the overarching goal of developing general-purpose intelligent machines. \n",
      "In this article, we propose an alternative approach: we first define the general characteristics we think intelligent machines should possess, and then we present a concrete roadmap to develop them in realistic, small steps, that are however incrementally structured in such a way that, jointly, they should lead us close to the ultimate goal of implementing a powerful AI. We realise that our vision of artificial intelligence and how to create it is just one among many. We focus here on a plan that, we hope, will lead to genuine progress, without by this implying that there are not other valid approaches to the task.\n",
      "The article is structured as follows. In Section 2 we indicate the two fundamental characteristics that we consider crucial for developing intelligence– at least the sort of intelligence we are interested in–namely communication and learning. Our goal is to build a machine that can learn new concepts through communication at a similar rate as a human with similar prior knowledge. That is, if one can easily learn how subtraction works after mastering addition, the intelligent machine, after grasping the concept of addition, should not find it difficult to learn subtraction as well.\n",
      "Since, as we said, achieving the long-term goal of building an intelligent machine equipped with the desired features at once seems too difficult, we need to define intermediate targets that can lead us in the right direction. We specify such targets in terms of simplified but self-contained versions of the final machine we want to develop. Our plan is to “educate” the target machine like a child: At any time in its development, the target machine should act like a stand-alone intelligent system, albeit one that will be initially very limited in what it can do. The bulk of our proposal (Section 3) thus consists in the plan for an interactive learning environment fostering the incremental development of progressively more intelligent behaviour. \n",
      "Section 4 briefly discusses some of the algorithmic capabilities we think a machine should possess in order to profitably exploit the learning environment. Finally, Section 5 situates our proposal in the broader context of past and current attempts to develop intelligent machines.\n",
      "Download the full paper here\n",
      "\n",
      "\n",
      "The Lovelace 2.0 Test asks whether a computer can create an artefact – a poem, story, painting or architectural design – that expert and unbiased observers would conclude was designed by a human. \n",
      "Prof. Mark Riedl proposes the concept of artificial creativity, akin to artificial intelligence. This could be tested he says, using an alternative to the Turing Test, the AI benchmark that asserts that if a computer system can fool a human being into thinking it is human itself, then it can be said to be truly intelligent. \n",
      "You can read Prof. Riedl’s paper below: \n",
      "Abstract\n",
      "Observing that the creation of certain types of artistic artefacts necessitate intelligence, we present the Lovelace 2.0 Test of creativity as an alternative to the Turing Test as a means of determining whether an agent is intelligent.\n",
      "The Lovelace 2.0 Test builds off prior tests of creativity and additionally provides a means of directly comparing the relative intelligence of different agents.\n",
      "Mark O. Riedl\n",
      "School of Interactive Computing; Georgia Institute of Technology\n",
      "riedl@cc.gatech.edu\n",
      "Download the full paper here\n",
      "\n",
      "\n",
      "Abstract\n",
      "Recurrent neural networks are convenient and efficient models for language modeling. However, when applied on the level of characters instead of words, they suffer from several problems. In order to successfully model long-term dependencies, the hidden representation needs to be large. This in turn implies higher computational costs, which can become prohibitive in practice. We propose two alternative structural modifications to the classical RNN model. The first one consists on conditioning the character level representation on the previous word representation. The other one uses the character history to condition the output probability. We evaluate the performance of the two proposed modifications on challenging, multi-lingual real world data.\n",
      "Piotr Bojanowski ∗\n",
      "INRIA\n",
      "Paris, France\n",
      "piotr.bojanowski@inria.fr\n",
      "Armand Joulin and Tomas Mikolov\n",
      "Facebook AI Research\n",
      "New York, NY, USA\n",
      "tmikolov.ajoulin@fb.com\n",
      "Download the full paper here\n",
      "\n",
      "\n",
      "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed- forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output.\n",
      "Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.\n",
      "INTRODUCTION\n",
      "New machine learning algorithms can lead to significant advances in automatic speech recognition. The biggest single advance occured nearly four decades ago with the introduction of the Expectation-Maximization (EM) algorithm for training Hidden Markov Models (HMMs). With the EM algorithm, it became possible to develop speech recognition systems for real world tasks using the richness of Gaussian mixture models (GMM) to represent the relationship between HMM states and the acoustic input. In these systems the acoustic input is typically represented by concatenating Mel Frequency Cepstral Coefficients (MFCCs) or Perceptual Linear Predictive coefficients (PLPs) computed from the raw waveform, and their first- and second-order temporal differences. This non-adaptive but highly- engineered pre-processing of the waveform is designed to discard the large amount of information in waveforms that is considered to be irrelevant for discrimination and to express the remaining information in a form that facilitates discrimination with GMM-HMMs.\n",
      "GMMs have a number of advantages that make them suitable for modeling the probability distributions over vectors of input features that are associated with each state of an HMM. With enough components, they can model probability distributions to any required level of accuracy and they are fairly easy to fit to data using the EM algorithm. A huge amount of research has gone into ways of constraining GMMs to increase their evaluation speed and to optimize the trade-off between their flexibility and the amount of training data available to avoid serious overfitting.\n",
      "The recognition accuracy of a GMM-HMM system can be further improved if it is discriminatively fine-tuned after it has been generatively trained to maximize its probability of generating the observed data, especially if the discriminative objective function used for training is closely related to the error rate on phones, words or sentences[7]. The accuracy can also be improved by augmenting (or concatenating) the input features (e.g., MFCCs) with “tandem” or bottleneck features generated using neural networks. GMMs are so successful that it is difficult for any new method to outperform them for acoustic modeling.\n",
      "Despite all their advantages, GMMs have a serious shortcoming – they are statistically inefficient for modeling data that lie on or near a non-linear manifold in the data space. For example, modeling the set of points that lie very close to the surface of a sphere only requires a few parameters using an appropriate model class, but it requires a very large number of diagonal Gaussians or a fairly large number of full-covariance Gaussians. Speech is produced by modulating a relatively small number of parameters of a dynamical system [10], [11] and this implies that its true underlying structure is much lower-dimensional than is immediately apparent in a window that contains hundreds of coefficients. We believe, therefore, that other types of model may work better than GMMs for acoustic modeling if they can more effectively exploit information embedded in a large window of frames.\n",
      "Artificial neural networks trained by backpropagating error derivatives have the potential to learn much better models of data that lie on or near a non-linear manifold. In fact two decades ago, researchers achieved some success using artificial neural networks with a single layer of non-linear hidden units to predict HMM states from windows of acoustic coefficients. At that time, however, neither the hardware nor the learning algorithms were adequate for training neural networks with many hidden layers on large amounts of data and the performance benefits of using neural networks with a single hidden layer were not sufficiently large to seriously challenge GMMs. As a result, the main practical contribution of neural networks at that time was to provide extra features in tandem or bottleneck systems.\n",
      "Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (DNNs) that contain many layers of non-linear hidden units and a very large output layer. The large output layer is required to accommodate the large number of HMM states that arise when each phone is modelled by a number of different “triphone” HMMs that take into account the phones on either side. Even when many of the states of these triphone HMMs are tied together, there can be thousands of tied states. Using the new learning methods, several different research groups have shown that DNNs can outperform GMMs at acoustic modeling for speech recognition on a variety of datasets including large datasets with large vocabularies.\n",
      "This review paper aims to represent the shared views of research groups at the University of Toronto, Microsoft Research (MSR), Google and IBM Research, who have all had recent successes in using DNNs for acoustic modeling. The paper starts by describing the two-stage training procedure that is used for fitting the DNNs. In the first stage, layers of feature detectors are initialized, one layer at a time, by fitting a stack of generative models, each of which has one layer of latent variables. These generative models are trained without using any information about the HMM states that the acoustic model will need to discriminate. In the second stage, each generative model in the stack is used to initialize one layer of hidden units in a DNN and the whole network is then discriminatively fine-tuned to predict the target HMM states. These targets are obtained by using a baseline GMM-HMM system to produce a forced alignment.\n",
      "In this paper we review exploratory experiments on the TIMIT database that were used to demonstrate the power of this two-stage training procedure for acoustic modeling. The DNNs that worked well on TIMIT were then applied to five different large vocabulary, continuous speech recognition tasks by three different research groups whose results we also summarize. The DNNs worked well on all of these tasks when compared with highly-tuned GMM-HMM systems and on some of the tasks they outperformed the state-of-the-art by a large margin. We also describe some other uses of DNNs for acoustic modeling and some variations on the training procedure.\n",
      "View the full PDF publication here.\n",
      "[Geoffrey Hinton, Li Deng, Dong Yu, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, George Dahl, and Brian Kingsbury]\n",
      "\n",
      "\n",
      "David Chalmers is a leading philosopher of mind, and the first to publish a major philosophy journal article on the singularity:\n",
      "Chalmers, D. (2010). “The Singularity: A Philosophical Analysis.” Journal of Consciousness Studies 17:7-65.\n",
      "Chalmers’ article is a “survey” article in that it doesn’t cover any arguments in depth, but quickly surveys a large number of positions and arguments in order to give the reader a “lay of the land.” Because of this, Chalmers’ paper is a remarkably broad and clear introduction to the singularity.\n",
      "Singularitarian authors will also be pleased that they can now cite a peer-reviewed article by a leading philosopher of mind who takes the singularity seriously.\n",
      "Below is a CliffsNotes of the paper for those who don’t have time to read all 58 pages of it.\n",
      "The Singularity: Is It Likely?\n",
      "Chalmers focuses on the “intelligence explosion” kind of singularity, and his first project is to formalise and defend I.J. Good’s 1965 argument. Defining AI as being “of human level intelligence,” AI+ as AI “of greater than human level” and AI++ as “AI of far greater than human level” (super intelligence), Chalmers updates Good’s argument to the following:\n",
      "\n",
      "There will be AI (before long, absent defeaters).\n",
      "If there is AI, there will be AI+ (soon after, absent defeaters).\n",
      "If there is AI+, there will be AI++ (soon after, absent defeaters).\n",
      "Therefore, there will be AI++ (before too long, absent defeaters).\n",
      "\n",
      "By “defeaters,” Chalmers means global catastrophes like nuclear war or a major asteroid impact. One way to satisfy premise (1) is to achieve AI through brain emulation (Sandberg & Bostrom, 2008). Against this suggestion, Lucas (1961), Dreyfus (1972), and Penrose (1994) argue that human cognition is not the sort of thing that could be emulated. Chalmers (1995; 1996, chapter 9) has responded to these criticisms at length. Briefly, Chalmers notes that even if the brain is not a rule-following algorithmic symbol system, we can still emulate it if it is mechanical. (Some say the brain is not mechanical, but Chalmers dismisses this as being discordant with the evidence.)\n",
      "Searle (1980) and Block (1981) argue instead that even if we can emulate the human brain, it doesn’t follow that the emulation is intelligent or has a mind. Chalmers says we can set these concerns aside by stipulating that when discussing the singularity, AI need only be measured in terms of behavior. The conclusion that there will be AI++ at least in this sense would still be massively important.\n",
      "Another consideration in favor of premise (1) is that evolution produced human-level intelligence, so we should be able to build it, too. Perhaps we will even achieve human-level AI by evolving a population of dumber AIs through variation and selection in virtual worlds. We might also achieve human-level AI by direct programming or, more likely, systems of machine learning.\n",
      "Premise (2) is plausible because AI will probably be produced by an extendible method, and so extending that method will yield AI+. Brain emulation might turn out not to be extendible, but the other methods are. Even if human-level AI is first created by a non-extendible method, this method itself would soon lead to an extendible method, and in turn enable AI+. AI+ could also be achieved by direct brain enhancement.\n",
      "Premise (3) is the amplification argument from Good: an AI+ would be better than we are at designing intelligent machines, and could thus improve its own intelligence. Having done that, it would be even better at improving its intelligence. And so on, in a rapid explosion of intelligence.\n",
      "In section 3 of his paper, Chalmers argues that there could be an intelligence explosion without there being such a thing as “general intelligence” that could be measured, but I won’t cover that here.\n",
      "In section 4, Chalmers lists several possible obstacles to the singularity.\n",
      "Constraining AI\n",
      "Next, Chalmers considers how we might design an AI+ that helps to create a desirable future and not a horrifying one. If we achieve AI+ by extending the method of human brain emulation, the AI+ will at least begin with something like our values. Directly programming friendly values into an AI+ (Yudkowsky, 2004) might also be feasible, though an AI+ arrived at by evolutionary algorithms is worrying.\n",
      "Most of this assumes that values are independent of intelligence, as Hume argued. But if Hume was wrong and Kant was right, then we will be less able to constrain the values of a superintelligent machine, but the more rational the machine is, the better values it will have.\n",
      "Another way to constrain an AI is not internal but external. For example, we could lock it in a virtual world from which it could not escape, and in this way create a leakproof singularity. But there is a problem. For the AI to be of use to us, some information must leak out of the virtual world for us to observe it. But then, the singularity is not leakproof. And if the AI can communicate us, it could reverse-engineer human psychology from within its virtual world and persuade us to let it out of its box – into the internet, for example.\n",
      "Our Place in a Post-Singularity World\n",
      "Chalmers says there are four options for us in a post-singularity world: extinction, isolation, inferiority, and integration.\n",
      "The first option is undesirable. The second option would keep us isolated from the AI, a kind of technological isolationism in which one world is blind to progress in the other. The third option may be infeasible because an AI++ would operate so much faster than us that inferiority is only a blink of time on the way to extinction.\n",
      "For the fourth option to work, we would need to become superintelligent machines ourselves. One path to this mind bemind uploading, which comes in several varieties and has implications for our notions of consciousness and personal identity that Chalmers discusses but I will not. (Short story: Chalmers prefers gradual uploading, and considers it a form of survival.)\n",
      "Conclusion\n",
      "Chalmers concludes:\n",
      "Will there be a singularity? I think that it is certainly not out of the question, and that the main obstacles are likely to be obstacles of motivation rather than obstacles of capacity.\n",
      "How should we negotiate the singularity? Very carefully, by building appropriate values into machines, and by building the first AI and AI+ systems in virtual worlds.\n",
      "How can we integrate into a post-singularity world? By gradual uploading followed by enhancement if we are still around then, and by reconstructive uploading followed by enhancement if we are not.\n",
      "References\n",
      "Block (1981). “Psychologism and behaviorism.” Philosophical Review 90:5-43.\n",
      "Chalmers (1995). “Minds, machines, and mathematics.” Psyche 2:11-20.\n",
      "Chalmers (1996). The Conscious Mind. Oxford University Press.\n",
      "Dreyfus (1972). What Computers Can’t Do. Harper & Row.\n",
      "Lucas (1961). “Minds, machines, and Godel.” Philosophy 36:112-27.\n",
      "Penrose (1994). Shadows of the Mind. Oxford University Press.\n",
      "Sandberg & Bostrom (2008). “Whole brain emulation: A roadmap.” Technical report 2008-3, Future for Humanity Institute, Oxford University.\n",
      "Searle (1980). “Minds, brains, and programs.” Behavioral and Brain Sciences 3:417-57.\n",
      "Yudkowsky (2004). “Coherent Extrapolated Volition.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for abstract in soup.find_all('div', class_='entry-content'):\n",
    "    print(abstract.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "July 21, 2019\n"
     ]
    }
   ],
   "source": [
    "date = soup.find('time', class_='entry-date published updated')\n",
    "print(date.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "July 21, 2019\n",
      "June 18, 2018\n"
     ]
    }
   ],
   "source": [
    "for date in soup.find_all('time', class_='entry-date published updated'):\n",
    "    print(date.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 26, 2018\n"
     ]
    }
   ],
   "source": [
    "date = soup.find('time', class_='entry-date published')\n",
    "print(date.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 26, 2018\n",
      "December 25, 2017\n",
      "October 26, 2016\n",
      "January 11, 2016\n",
      "January 5, 2016\n",
      "December 30, 2015\n",
      "October 23, 2015\n",
      "September 16, 2015\n"
     ]
    }
   ],
   "source": [
    "for date in soup.find_all('time', class_='entry-date published'):\n",
    "    print(date.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"screen-reader-text\">Search for:</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\"> on Explained: XLNet – Generalized Autoregressive Pretraining for Language Understanding</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\">Tags </span>, <span class=\"screen-reader-text\"> on Overcoming Priors for Visual Question Answering</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\">Tags </span>, <span class=\"screen-reader-text\"> on The importance of single directions for generalization</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\">Tags </span>, <span class=\"screen-reader-text\"> on SingularityNET: A decentralized, open market and inter-network for AIs</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\"> on WaveNet: A Generative Model for Raw Audio</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\">Tags </span>, <span class=\"screen-reader-text\"> on Facebook AI Research – Machine Intelligence Roadmap</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\">Tags </span>, <span class=\"screen-reader-text\"> on The Lovelace 2.0 Test of Artificial Creativity and Intelligence</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\">Tags </span>, <span class=\"screen-reader-text\"> on Alternative structures for character-level RNNs</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\">Tags </span>, <span class=\"screen-reader-text\"> on Deep Neural Networks for Acoustic Modeling in Speech Recognition</span>, <span class=\"screen-reader-text\">Posted on </span>, <span class=\"screen-reader-text\">Categories </span>, <span class=\"screen-reader-text\">Tags </span>, <span class=\"screen-reader-text\"> on The Singularity: A Philosophical Analysis</span>]\n"
     ]
    }
   ],
   "source": [
    "area = soup.find_all('span', class_='screen-reader-text')\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for:\n",
      "Posted on \n",
      "Categories \n",
      " on Explained: XLNet – Generalized Autoregressive Pretraining for Language Understanding\n",
      "Posted on \n",
      "Categories \n",
      "Tags \n",
      " on Overcoming Priors for Visual Question Answering\n",
      "Posted on \n",
      "Categories \n",
      "Tags \n",
      " on The importance of single directions for generalization\n",
      "Posted on \n",
      "Categories \n",
      "Tags \n",
      " on SingularityNET: A decentralized, open market and inter-network for AIs\n",
      "Posted on \n",
      "Categories \n",
      " on WaveNet: A Generative Model for Raw Audio\n",
      "Posted on \n",
      "Categories \n",
      "Tags \n",
      " on Facebook AI Research – Machine Intelligence Roadmap\n",
      "Posted on \n",
      "Categories \n",
      "Tags \n",
      " on The Lovelace 2.0 Test of Artificial Creativity and Intelligence\n",
      "Posted on \n",
      "Categories \n",
      "Tags \n",
      " on Alternative structures for character-level RNNs\n",
      "Posted on \n",
      "Categories \n",
      "Tags \n",
      " on Deep Neural Networks for Acoustic Modeling in Speech Recognition\n",
      "Posted on \n",
      "Categories \n",
      "Tags \n",
      " on The Singularity: A Philosophical Analysis\n"
     ]
    }
   ],
   "source": [
    "for area in soup.find_all('span', class_='screen-reader-text'):\n",
    "    print(area.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
